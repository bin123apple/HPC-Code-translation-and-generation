SUBROUTINE freeData() deallocate(alltoallSendBuf) deallocate(alltoallRecvBuf) deallocate(alltoallFinalBuf) END SUBROUTINE freeData
SUBROUTINE testAlltoall(dataSize) integer, intent(in) :: dataSize integer :: sizeofBuffer, i,j integer :: dataForEachThread, startElem logical :: testFlag, reduceFlag testFlag = .true. sizeofBuffer = dataSize * numThreads * numMPIprocs * numThreads allocate(testBuf(sizeofBuffer)) dataForEachThread = dataSize * numThreads * numMPIProcs startElem = (myThreadID - 1)* dataForEachThread DO i = 1, (numThreads * numMPIprocs) DO j = 1, dataSize testBuf(startElem + (i-1)*dataSize + j) = i END DO END DO DO i = 1, sizeofBuffer IF(alltoallFinalBuf(i) /= testBuf(i)) THEN testFlag = .false. END IF END DO CALL MPI_Reduce(testFlag, reduceFlag, 1, MPI_LOGICAL, & MPI_LAND, 0, comm, ierr) IF (myMPIRank == 0) THEN CALL setTestOutcome(reduceFlag) END IF deallocate(testBuf) END SUBROUTINE testAlltoall
SUBROUTINE barrierDriver() repsToDo = defaultReps CALL barrierKernel(warmUpIters) benchComplete = .false. DO WHILE (benchComplete .NEQV. .true.) CALL MPI_Barrier(comm,ierr) startTime = MPI_Wtime() CALL barrierKernel(repsToDo) CALL MPI_Barrier(comm, ierr) finishTime = MPI_Wtime() totalTime = finishTime - startTime if (myMPIRank==0) then benchComplete = repTimeCheck(totalTime, repsToDo) end if call MPI_Bcast(benchComplete, 1, MPI_INTEGER, 0, comm, ierr) call MPI_Bcast(repsToDo, 1, MPI_INTEGER, 0, comm, ierr) END DO 
SUBROUTINE barrierKernel(totalReps) integer, intent(in) :: totalReps integer :: repIter DO repIter = 1, totalReps CALL MPI_Barrier(comm, ierr) END DO END SUBROUTINE barrierKernel
SUBROUTINE broadcast() integer :: dataSizeIter integer :: sizeofFinalBuf repsToDo = defaultReps dataSizeIter = minDataSize DO WHILE (dataSizeIter <= maxDataSize) CALL allocateData(dataSizeIter) CALL broadcastKernel(warmUpIters,dataSizeIter) sizeofFinalBuf = dataSizeIter * numThreads CALL testBroadcast(sizeofFinalBuf) benchComplete = .false. DO WHILE (benchComplete .NEQV. .true.) CALL MPI_Barrier(comm, ierr) startTime = MPI_WTime() CALL broadcastKernel(repsToDo, dataSizeIter) CALL MPI_Barrier(comm, ierr) finishTime = MPI_Wtime() totalTime = finishTime - startTime if (myMPIRank==0) then benchComplete = repTimeCheck(totalTime, repsToDo) end if call MPI_Bcast(benchComplete, 1, MPI_INTEGER, 0, comm, ierr) call MPI_Bcast(repsToDo, 1, MPI_INTEGER, 0, comm, ierr) END DO IF (myMPIRank == 0) THEN CALL setReportParams(dataSizeIter,repsToDo,totalTime) CALL printReport() END IF CALL freeData() dataSizeIter = dataSizeIter * 2 END DO END SUBROUTINE broadcast
SUBROUTINE broadcastKernel(totalReps, dataSize) integer, intent(in) :: totalReps, dataSize integer :: repIter, i integer, parameter :: BROADCASTROOT = 0 integer :: startPos DO repIter = 1, totalReps IF (myMPIRank == BROADCASTROOT) THEN DO i = 1, dataSize broadcastBuf(i) = BROADCASTNUM END DO END IF CALL MPI_Bcast(broadcastBuf, dataSize, MPI_INTEGER, & BROADCASTROOT, comm, ierr) startPos = ((myThreadID-1) * dataSize) DO i = 1, dataSize finalBroadcastBuf(startPos + i) = broadcastBuf(i) END DO END DO END SUBROUTINE broadcastKernel
SUBROUTINE allocateData(bufferSize) integer, intent(in) :: bufferSize allocate(broadcastBuf(bufferSize)) allocate(finalBroadcastBuf(bufferSize*numThreads)) END SUBROUTINE allocateData
SUBROUTINE freeData() deallocate(broadcastBuf) deallocate(finalBroadcastBuf) END SUBROUTINE freeData
SUBROUTINE testBroadcast(bufferSize) integer, intent(in) :: bufferSize integer :: i logical :: testFlag, reduceFlag testFlag = .true. DO i = 1, bufferSize IF (finalBroadcastBuf(i) /= BROADCASTNUM) THEN testFlag = .false. END IF END DO CALL MPI_Reduce(testFlag, reduceFlag, 1, MPI_LOGICAL, & MPI_LAND, 0, comm, ierr) IF (myMPIRank == 0) THEN CALL setTestOutcome(reduceFlag) END IF END SUBROUTINE testBroadcast
SUBROUTINE reduction(benchmarkType) integer, intent(in) :: benchmarkType integer :: dataSizeIter integer :: sizeofBuf repsToDo = defaultReps dataSizeIter = minDataSize DO WHILE (dataSizeIter <= maxDataSize) CALL allocateData(dataSizeIter) IF (benchmarkType == REDUCE) THEN CALL reduceKernel(warmUpIters,dataSizeIter) IF (myMPIRank == 0) THEN CALL testReduce(dataSizeIter,benchmarkType) END IF ELSE IF (benchmarkType == ALLREDUCE) THEN sizeofBuf = dataSizeIter * numThreads CALL allReduceKernel(warmUpIters, dataSizeIter) CALL testReduce(sizeofBuf,benchmarkType) END IF benchComplete = .false. DO WHILE (benchComplete .NEQV. .true.) CALL MPI_Barrier(comm, ierr) startTime = MPI_WTime() IF (benchmarkType == REDUCE) THEN CALL reduceKernel(repsToDo, dataSizeIter) ELSE IF (benchmarkType == ALLREDUCE) THEN CALL allReduceKernel(repsToDo, dataSizeIter) END IF CALL MPI_Barrier(comm, ierr) finishTime = MPI_WTime() totalTime = finishTime - startTime if (myMPIRank==0) then benchComplete = repTimeCheck(totalTime, repsToDo) end if call MPI_Bcast(benchComplete, 1, MPI_INTEGER, 0, comm, ierr) call MPI_Bcast(repsToDo, 1, MPI_INTEGER, 0, comm, ierr) END DO IF (myMPIRank == 0) THEN CALL setReportParams(dataSizeIter,repsToDo,totalTime) CALL printReport() END IF CALL freeData() dataSizeIter = dataSizeIter * 2 END DO END SUBROUTINE reduction
SUBROUTINE reduceKernel(totalReps, dataSize) integer, intent(in) :: totalReps, dataSize integer :: repIter,i integer, dimension(dataSize) :: localReduceBuf DO repIter = 1, totalReps localReduceBuf = 0 globalReduceBuf = 0 finalReduceBuf = 0 DO i = 1, dataSize localReduceBuf(i) = localReduceBuf(i) + globalIDarray(myThreadID) END DO CALL MPI_Reduce(localReduceBuf, globalReduceBuf, & dataSize, MPI_INTEGER, MPI_SUM, 0, comm, ierr) if (myMPIRank==0) then finalReduceBuf(1:dataSize) = globalReduceBuf end if END DO END SUBROUTINE reduceKernel
SUBROUTINE allReduceKernel(totalReps, dataSize) integer, intent(in) :: totalReps, dataSize integer :: repIter,i integer :: startPos integer, dimension(dataSize) :: localReduceBuf DO repIter = 1, totalReps localReduceBuf = 0 globalReduceBuf = 0 finalReduceBuf = 0 DO i = 1, dataSize localReduceBuf(i) = localReduceBuf(i) + globalIDarray(myThreadID) END DO CALL MPI_Allreduce(localReduceBuf, globalReduceBuf, & dataSize, MPI_INTEGER, MPI_SUM, comm, ierr) startPos = ((myThreadID-1) * dataSize) DO i = 1, dataSize finalReduceBuf(startPos + i) = globalReduceBuf(i) END DO END DO END SUBROUTINE allReduceKernel
SUBROUTINE allocateData(bufferSize) integer, intent(in) :: bufferSize allocate(globalReduceBuf(bufferSize)) allocate(finalReduceBuf(bufferSize*numThreads)) END SUBROUTINE allocateData
SUBROUTINE freeData() deallocate(globalReduceBuf) deallocate(finalReduceBuf) END SUBROUTINE freeData
SUBROUTINE testReduce(bufferSize,benchmarkType) integer, intent(in) :: bufferSize, benchmarkType integer :: i integer :: correctReduce, lastGlobalID logical :: testFlag, reduceFlag correctReduce = 0 testFlag = .true. lastGlobalID = (numMPIprocs * numThreads) DO i = 1, lastGlobalID correctReduce = correctReduce + i END DO DO i = 1, bufferSize IF (finalReduceBuf(i) /= correctReduce) THEN testFlag = .false. END IF END DO IF (benchmarkType == ALLREDUCE) THEN CALL MPI_Reduce(testFlag, reduceFlag, 1, MPI_LOGICAL, & MPI_LAND, 0, comm, ierr) IF (myMPIRank == 0) THEN CALL setTestOutcome(reduceFlag) END IF ELSE CALL setTestOutcome(testFlag) END IF END SUBROUTINE testReduce
SUBROUTINE scatterGather(benchmarkType) integer, intent(in) :: benchmarkType integer :: dataSizeIter integer :: bufferSize repsToDo = defaultReps dataSizeIter = minDataSize DO WHILE (dataSizeIter <= maxDataSize) bufferSize = dataSizeIter * numThreads IF (benchmarkType == SCATTER) THEN CALL allocateData(bufferSize,benchmarkType) CALL scatterKernel(warmUpIters, dataSizeIter) CALL testScatterGather(bufferSize, benchmarkType) ELSE IF (benchmarkType == GATHER) THEN CALL allocateData(bufferSize,benchmarkType) CALL gatherKernel(warmUpIters, dataSizeIter) IF (myMPIRank == GATHERROOT) THEN CALL testScatterGather(bufferSize*numMPIprocs, benchmarkType) END IF END IF benchComplete = .false. DO WHILE (benchComplete .NEQV. .true.) CALL MPI_Barrier(comm, ierr) startTime = MPI_Wtime() IF (benchmarkType == SCATTER) THEN CALL scatterKernel(repsToDo, dataSizeIter) ELSE IF (benchmarkType == GATHER) THEN CALL gatherKernel(repsToDo, dataSizeIter) END IF CALL MPI_Barrier(comm, ierr) finishTime = MPI_Wtime() totalTime = finishTime - startTime if (myMPIRank==0) then benchComplete = repTimeCheck(totalTime, repsToDo) end if call MPI_Bcast(benchComplete, 1, MPI_INTEGER, 0, comm, ierr) call MPI_Bcast(repsToDo, 1, MPI_INTEGER, 0, comm, ierr) END DO IF (myMPIRank == 0) THEN CALL setReportParams(dataSizeIter,repsToDo,totalTime) CALL printReport() END IF CALL freeData(benchmarkType) dataSizeIter = dataSizeIter * 2 END DO END SUBROUTINE scatterGather
SUBROUTINE scatterKernel(totalReps, dataSize) integer, intent(in) :: totalReps, dataSize integer :: repIter, i integer :: totalSendBufElems, sendCount, recvCount totalSendBufElems = numMPIprocs * numThreads * dataSize sendCount = dataSize * numThreads recvCount = sendCount DO repIter = 1, totalReps IF (myMPIRank == SCATTERROOT) THEN DO i = 1, totalSendBufElems scatterSendBuf(i) = SCATTERSTARTVAL + i END DO END IF CALL MPI_Scatter(scatterSendBuf, sendCount, MPI_INTEGER, & scatterRecvBuf, recvCount, MPI_INTEGER, & SCATTERROOT, comm, ierr) DO i = 1, recvCount finalBuf(i) = scatterRecvBuf(i) END DO END DO END SUBROUTINE scatterKernel
SUBROUTINE gatherKernel(totalReps, dataSize) integer, intent(in) :: totalReps, dataSize integer :: repIter, i integer :: totalRecvBufElems integer :: sendCount, recvCount integer :: startVal totalRecvBufElems = dataSize * numThreads * numMPIprocs sendCount = dataSize * numThreads recvCount = sendCount startVal = (myMPIRank * sendCount) + GATHERSTARTVAL DO repIter = 1, totalReps DO i = 1, sendCount gatherSendBuf(i) = startVal + i END DO CALL MPI_Gather(gatherSendBuf, sendCount, MPI_INTEGER, & gatherRecvBuf, recvCount, MPI_INTEGER, & GATHERROOT, comm, ierr) IF (myMPIRank == GATHERROOT) THEN DO i = 1, totalRecvBufElems finalBuf(i) = gatherRecvBuf(i) END DO END IF END DO END SUBROUTINE gatherKernel
SUBROUTINE allocateData(bufferSize, benchmarkType) integer, intent(in) :: bufferSize, benchmarkType IF (benchmarkType == SCATTER) THEN IF (myMPIRank == SCATTERROOT) THEN allocate(scatterSendBuf(bufferSize*numMPIprocs)) END IF allocate(scatterRecvBuf(bufferSize)) allocate(finalBuf(bufferSize)) ELSE IF (benchmarkType == GATHER) THEN allocate(gatherSendBuf(bufferSize)) IF (myMPIRank == GATHERROOT) THEN allocate(gatherRecvBuf(bufferSize*numMPIprocs)) allocate(finalBuf(bufferSize*numMPIprocs)) END IF END IF END SUBROUTINE allocateData
SUBROUTINE freeData(benchmarkType) integer, intent(in) :: benchmarkType IF (benchmarkType == SCATTER) THEN IF (myMPIRank == SCATTERROOT) THEN deallocate(scatterSendBuf) END IF deallocate(scatterRecvBuf) deallocate(finalBuf) ELSE IF (benchmarkType == GATHER) THEN deallocate(gatherSendBuf) IF (myMPIRank == GATHERROOT) THEN deallocate(gatherRecvBuf) deallocate(finalBuf) END IF END IF END SUBROUTINE freeData
SUBROUTINE testScatterGather(sizeofBuffer, benchmarkType) integer, intent(in) :: sizeofBuffer, benchmarkType integer :: i integer :: startVal logical :: testFlag, reduceFlag testFlag = .true. allocate(testBuf(sizeofBuffer)) IF (benchmarkType == SCATTER) THEN startVal = (myMPIRank*sizeofBuffer) + SCATTERSTARTVAL ELSE IF (benchmarkType == GATHER) THEN startVal = GATHERSTARTVAL END IF DO i = 1, sizeofBuffer testBuf(i) = startVal + i END DO DO i = 1, sizeofBuffer IF (finalBuf(i) /= testBuf(i)) THEN testFlag = .false. END IF END DO IF (benchmarkType == SCATTER) THEN CALL MPI_Reduce(testFlag, reduceFlag, 1, MPI_LOGICAL, & MPI_LAND, 0, comm, ierr) IF (myMPIRank == 0) THEN CALL setTestOutcome(reduceFlag) END IF ELSE IF (benchmarkType == GATHER) THEN CALL setTestOutcome(testFlag) END IF deallocate(testBuf) END SUBROUTINE testScatterGather
PROGRAM mixedModeBenchmark use pt_to_pt_pingpong use pt_to_pt_pingping use pt_to_pt_multiPingPong use pt_to_pt_multiPingPing use pt_to_pt_haloExchange use collective_barrier use collective_reduction use collective_broadcast use collective_scatterGather use collective_alltoall use parallelEnvironment use benchmarkSetup use output implicit none character (len = MAXSTRING) :: name logical :: supportFlag CALL initParallelEnv() IF (myMPIRank == 0) THEN CALL printHeader(numMPIprocs,numThreads,threadSupport) CALL openFile() CALL setupBenchmarkList() END IF CALL readBenchmarkParams() CALL findBenchmarkNumber() DO WHILE(benchmarkNumber /= FINISHED) benchmarks : SELECT CASE (benchmarkNumber) CASE(1) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED) name = "Masteronly Pingpong" CALL setBenchName(name, benchmarkNumber, supportFlag) END IF CALL pingPong(MASTERONLY) CASE(2) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED) name = "Funnelled Pingpong" CALL setBenchName(name, benchmarkNumber, supportFlag) END IF CALL pingPong(FUNNELLED) CASE(3) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_MULTIPLE) name = "Multiple Pingpong" CALL setBenchName(name, benchmarkNumber, supportFlag) END IF CALL pingPong(MULTIPLE) CASE(4) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED) name = "Masteronly Pingping" CALL setBenchName(name, benchmarkNumber, supportFlag) END IF CALL pingPing(MASTERONLY) CASE(5) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED) name = "Funnelled Pingping" CALL setBenchName(name, benchmarkNumber, supportFlag) END IF CALL pingPing(FUNNELLED) CASE(6) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_MULTIPLE) name = "Multiple Pingping" CALL setBenchName(name, benchmarkNumber, supportFlag) END IF CALL pingPing(MULTIPLE) CASE(7) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED) name = "Masteronly Haloexchange" CALL setBenchName(name, benchmarkNumber, supportFlag) CALL printBenchHeader() END IF CALL haloExchange(MASTERONLY) CASE(8) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED) name = "Funnelled Haloexchange" CALL setBenchName(name, benchmarkNumber, supportFlag) CALL printBenchHeader() END IF CALL haloExchange(FUNNELLED) CASE(9) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_MULTIPLE) name = "Multiple Haloexchange" CALL setBenchName(name, benchmarkNumber, supportFlag) CALL printBenchHeader() END IF CALL haloExchange(MULTIPLE) CASE(10) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED) name = "Masteronly MultiPingpong" CALL setBenchName(name, benchmarkNumber, supportFlag) END IF CALL multiPingPong(MASTERONLY) CASE(11) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED) name = "Funnelled MultiPingpong" CALL setBenchName(name, benchmarkNumber, supportFlag) END IF CALL multiPingPong(FUNNELLED) CASE(12) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_MULTIPLE) name = "Multiple MultiPingpong" CALL setBenchName(name, benchmarkNumber, supportFlag) END IF CALL multiPingPong(MULTIPLE) CASE(13) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED) name = "Masteronly MultiPingping" CALL setBenchName(name, benchmarkNumber, supportFlag) END IF CALL multiPingPing(MASTERONLY) CASE(14) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED) name = "Funnelled MultiPingping" CALL setBenchName(name, benchmarkNumber, supportFlag) END IF CALL multiPingPing(FUNNELLED) CASE(15) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_MULTIPLE) name = "Multiple MultiPingping" CALL setBenchName(name, benchmarkNumber, supportFlag) END IF CALL multiPingPing(MULTIPLE) CASE(16) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED) name = "Barrier" CALL setBenchName(name, benchmarkNumber, supportFlag) CALL printBenchHeader() END IF CALL barrierDriver() CASE(17) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED) name = "Reduce" CALL setBenchName(name, benchmarkNumber, supportFlag) CALL printBenchHeader() END IF CALL reduction(REDUCE) CASE(18) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED) name = "All Reduce" CALL setBenchName(name, benchmarkNumber, supportFlag) CALL printBenchHeader() END IF CALL reduction(ALLREDUCE) CASE(19) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED) name = "Broadcast" CALL setBenchName(name, benchmarkNumber, supportFlag) CALL printBenchHeader() END IF CALL broadcast() CASE(20) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED) name = "Scatter" CALL setBenchName(name, benchmarkNumber, supportFlag) CALL printBenchHeader() END IF CALL scatterGather(SCATTER) CASE(21) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED) name = "Gather" CALL setBenchName(name, benchmarkNumber, supportFlag) CALL printBenchHeader() END IF CALL scatterGather(GATHER) CASE(22) IF (myMPIRank == 0) THEN supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED) name = "All to all" CALL setBenchName(name, benchmarkNumber, supportFlag) CALL printBenchHeader() END IF CALL alltoall() CASE default END SELECT benchmarks CALL findBenchmarkNumber() END DO CALL finaliseParallelEnv() IF (myMPIRank == 0) THEN CALL closeFile() END IF END PROGRAM mixedModeBenchmark                                
SUBROUTINE printHeader(numProcs, numThreads, threadSupport) integer, intent(in) :: numProcs, numThreads, threadSupport character (len = MAXSTRING) :: string CALL threadSupportToString(threadSupport, string) write(*,*) "----------------------------------------------" write(*,*) "  Mixed mode MPI/OpenMP benchmark suite v1.0  " write(*,*) "----------------------------------------------" write(*,*) " Number of MPI processes =", numProcs write(*,*) " Number of OpenMP threads =", numThreads write(*,*) " Thread support = ", trim(string) END SUBROUTINE printHeader
SUBROUTINE setBenchName(name,number,support) character (len = MAXSTRING), intent (in) :: name integer, intent(in) :: number logical, intent(in) :: support benchReport%benchName = name benchReport%benchNumber = number benchReport%supported = support CALL printBenchName() END SUBROUTINE setBenchName
SUBROUTINE setTestOutcome(outcome) logical, intent(in) :: outcome benchReport%testOutcome = outcome END SUBROUTINE setTestOutcome
SUBROUTINE setReportParams(size,reps,time) integer, intent(in) :: size, reps DOUBLE PRECISION, intent(in) :: time benchReport%dataSize = size benchReport%numReps = reps benchReport%benchTime = time benchReport%timePerRep = time/reps IF (benchReport%benchNumber <= LAST_PT_PT_ID) THEN benchReport%bytes = size * numThreads * sizeInteger ELSE IF (benchReport%benchNumber <= LASTMULTIPPID) THEN benchReport%bytes = size * numThreads * sizeInteger * localCommSize ELSE benchReport%bytes = size * sizeInteger END IF END SUBROUTINE setReportParams
SUBROUTINE printBenchName() write(*,*) "--------------------------------------------" write(*,*) "# ", benchReport%benchName write(*,*) "--------------------------------------------" IF (benchReport%supported .EQV. .false.) THEN write(*,*) "WARNING: Implementation does not ",& "support benchmark" END IF END SUBROUTINE printBenchName
SUBROUTINE printBenchHeader() write(*,fmt="(2x,a9,5x,a16,5x,a8,5x,a10,5x,a12,5x,a4)")& "Data Size","Msg Size (bytes)","No. Reps",& "Time (sec)","Time/Rep (s)","Test" write(*,fmt="(1x,a11,3x,a18,3x,a10,3x,a12,3x,a14,3x,a6)")& "-----------","------------------","----------",& "------------","--------------","------" END SUBROUTINE printBenchHeader
SUBROUTINE printNodeReport(sameNode,rankA,rankB) integer, intent(in) :: rankA, rankB logical, intent(in) :: sameNode IF (sameNode .EQV. .true.) THEN write(*,*) "Intra node benchmark between process",& rankA, "and process", rankB ELSE IF (sameNode .EQV. .false.) THEN write(*,*) "Inter node benchmark between process",& rankA, "and process", rankB END IF END SUBROUTINE printNodeReport
SUBROUTINE printMultiProcInfo(printNode, pairWorldRank, pairProcName) integer, intent(in) :: printNode, pairWorldRank character (len = MPI_MAX_PROCESSOR_NAME) :: pairProcName IF (crossCommRank == printNode) THEN print *, "MPI process ", myMPIRank, "on ", trim(myProcName), & " commumicating with MPI process ", pairWorldRank, & "on ", trim(pairProcName) END IF END SUBROUTINE printMultiProcInfo
SUBROUTINE printReport() character (len =4) testString IF(benchReport%testOutcome .EQV. .true.) THEN testString = "Pass" ELSE testString = "Fail" END IF write(*,fmt="('d',i10,5x,i16,5x,i8,5x,f10.6,4x,f14.9,5x,a4)")& benchReport%dataSize, benchReport%bytes,& benchReport%numReps,benchReport%benchTime,& benchReport%timePerRep,testString END SUBROUTINE printReport
SUBROUTINE printBalanceError() print *, "" print *, "ERROR: Nodes selected for this benchmark do not",& "have same number of MPI processes per node.", & "Skipping benchmark..." print *, "" END SUBROUTINE printBalanceError
SUBROUTINE threadSupportToString(threadSupport, string) integer, intent(in) :: threadSupport character (len = MAXSTRING), intent(out) :: string IF (threadSupport == MPI_THREAD_SINGLE) THEN string = "MPI_THREAD_SINGLE" ELSE IF (threadSupport == MPI_THREAD_FUNNELED) THEN string = "MPI_THREAD_FUNNELED" ELSE IF (threadSupport == MPI_THREAD_SERIALIZED) THEN string = "MPI_THREAD_SERIALIZED" ELSE IF (threadSupport == MPI_THREAD_MULTIPLE) THEN string = "MPI_THREAD_MULTIPLE" END IF END SUBROUTINE threadSupportToString
SUBROUTINE initParallelEnv() CALL MPI_Init_thread(MPI_THREAD_MULTIPLE,threadSupport,ierr) comm = MPI_COMM_WORLD CALL MPI_Comm_size(comm, numMPIprocs, ierr) CALL MPI_Comm_rank(comm, myMPIRank, ierr) CALL MPI_Type_size(MPI_INTEGER, sizeInteger, ierr) CALL  MPI_Get_processor_name(myProcName, procNameLen, ierr) CALL setupCommunicators() numThreads = omp_get_num_threads() myThreadID = omp_get_thread_num() + 1 !threadID from 1 to totalThreads allocate(globalIDarray(numThreads)) globalIDarray(myThreadID) = (myMPIRank * numThreads) + myThreadID END SUBROUTINE initParallelEnv
SUBROUTINE finaliseParallelEnv() CALL MPI_Finalize(ierr) deallocate(globalIDarray) END SUBROUTINE finaliseParallelEnv SUBROUTINE findNeighbourRanks() integer :: dims(1) logical, parameter :: PERIODS(1) = (/.true./), REORDER = .false. dims = 0 CALL MPI_Dims_Create(numMPIProcs,1,dims,ierr) CALL MPI_Cart_Create(comm,1,dims,PERIODS,REORDER, & commCart, ierr) CALL MPI_Cart_Shift(commCart, 0, 1, leftNeighbour, & rightNeighbour, ierr) END SUBROUTINE findNeighbourRanks
FUNCTION findRank(rankIn) integer, intent(in) ::rankIn integer :: findRank IF (rankin < 0) THEN findRank = numMPIprocs + rankin ELSE findRank = rankIn END IF IF (findRank > (numMPIprocs-1)) THEN findRank = numMPIprocs - 1 ELSE IF (findRank < 0) THEN findRank = 0 END IF END FUNCTION findRank
FUNCTION benchmarkSupport(required) integer, intent(in) :: required logical :: benchmarkSupport IF (required <= threadSupport) THEN benchmarkSupport = .true. ELSE benchmarkSupport = .false. END IF END FUNCTION benchmarkSupport
FUNCTION compareProcNames(rankA, rankB) integer, intent(in) :: rankA, rankB logical compareProcNames character (len = MPI_MAX_PROCESSOR_NAME) :: recvProcName IF (myMPIRank == rankB) THEN CALL MPI_Send(myProcName, MPI_MAX_PROCESSOR_NAME, & MPI_CHARACTER, rankA, tag, comm, ierr) ELSE IF (myMPIRank == rankA) THEN CALL MPI_Recv(recvProcName, MPI_MAX_PROCESSOR_NAME, & MPI_CHARACTER, rankB, tag, comm, status, ierr) IF (myProcName == recvProcName) THEN compareProcNames = .true. ELSE compareProcNames = .false. END IF END IF CALL MPI_Bcast(compareProcNames, 1, MPI_LOGICAL, rankA, & comm, ierr) END FUNCTION compareProcNames
SUBROUTINE setupCommunicators() integer :: procHash procHash = procNameToHash() CALL MPI_Comm_split(comm, procHash, 0, localComm, ierr) CALL MPI_Comm_rank(localComm, localCommRank, ierr) CALL MPI_Comm_size(localComm, localCommSize, ierr) CALL MPI_Comm_split(comm, localCommRank, 0, crossComm, ierr) CALL MPI_Comm_rank(crossComm, crossCommRank, ierr) END SUBROUTINE setupCommunicators
FUNCTION procNameToHash() integer :: procNameToHash integer :: i procNameToHash = 0 DO i = 1, procNameLen procNameToHash = 7 * procNameToHash + & ICHAR(myProcName(i:i)) END DO END FUNCTION procNameToHash
SUBROUTINE exchangeWorldRanks(nodeA, nodeB, otherWorldRank) integer, intent(in) :: nodeA, nodeB integer, intent(out) :: otherWorldRank integer :: destRank IF (crossCommRank == nodeA) THEN destRank = nodeB ELSE IF (crossCommRank == nodeB) THEN destRank = nodeA END IF IF (crossCommRank == nodeA .or. crossCommRank == nodeB) THEN CALL MPI_Isend(myMPIRank, 1, MPI_INTEGER, destRank, & tag, crossComm, requestID, ierr) CALL MPI_Recv(otherWorldRank, 1, MPI_INTEGER, destRank, & tag, crossComm, status, ierr) CALL MPI_Wait(requestID, status, ierr) END IF END SUBROUTINE exchangeWorldRanks
SUBROUTINE sendProcName(destNode, srcNode, destProcName) integer, intent(in) :: srcNode, destNode character (len = MPI_MAX_PROCESSOR_NAME), intent(out) :: destProcName IF (crossCommRank == srcNode) THEN CALL MPI_Send(myProcName, MPI_MAX_PROCESSOR_NAME, & MPI_CHARACTER, destNode, tag, crossComm, ierr) ELSE IF (crossCommRank == destNode) THEN CALL MPI_Recv(destProcName, MPI_MAX_PROCESSOR_NAME, & MPI_CHARACTER, srcNode, tag, crossComm, status, ierr) END IF END SUBROUTINE sendProcName
FUNCTION crossCommBalance(nodeA, nodeB) integer, intent(in) :: nodeA, nodeB integer :: localCommSize, otherLocalCommSize logical :: crossCommBalance CALL MPI_Comm_size(localComm, localCommSize, ierr) IF (crossCommRank == nodeB .and. localCommRank == 0) THEN CALL MPI_Send(localCommSize, 1, MPI_INTEGER, nodeA, & tag, crossComm, ierr) ELSEIF (crossCommRank == nodeA .and. localCommRank == 0) THEN CALL MPI_Recv(otherLocalCommSize, 1, MPI_INTEGER, nodeB, & tag, crossComm, status, ierr) IF (localCommSize == otherLocalCommSize) THEN crossCommBalance = .true. ELSE crossCommBalance = .false. END IF IF (myMPIRank /= 0) THEN CALL MPI_Send(crossCommBalance, 1, MPI_LOGICAL, & 0, tag, comm, ierr) END IF END IF IF (myMPIRank == 0) THEN IF (crossCommRank /= nodeA .and. localCommRank /= 0) THEN CALL MPI_Recv(crossCommBalance, 1, MPI_LOGICAL, & MPI_ANY_SOURCE, tag, comm, status, ierr) END IF END IF CALL MPI_Bcast(crossCommBalance, 1, MPI_LOGICAL, 0, comm, ierr) END FUNCTION crossCommBalance
SUBROUTINE haloExchange(benchmarkType) integer, intent(in) :: benchmarkType integer :: dataSizeIter CALL findNeighbourRanks() repsToDo = defaultReps dataSizeIter = minDataSize DO WHILE (dataSizeIter <= maxDataSize) sizeofBuffer = dataSizeIter * numThreads CALL allocateData(sizeofBuffer) IF (benchmarkType == MASTERONLY) THEN CALL masteronlyHaloexchange(warmUpIters, dataSizeIter) ELSE IF (benchmarkType == FUNNELLED) THEN CALL funnelledHaloexchange(warmUpIters, dataSizeIter) ELSE IF (benchmarkType == MULTIPLE) THEN CALL multipleHaloexchange(warmUpIters, dataSizeIter) END IF CALL testHaloexchange(sizeofBuffer, dataSizeIter) benchComplete = .false. DO WHILE (benchComplete .NEQV. .true.) CALL MPI_Barrier(comm, ierr) startTime = MPI_Wtime() IF (benchmarkType == MASTERONLY) THEN CALL masteronlyHaloexchange(repsToDo, dataSizeIter) ELSE IF (benchmarkType == FUNNELLED) THEN CALL funnelledHaloexchange(repsToDo, dataSizeIter) ELSE IF (benchmarkType == MULTIPLE) THEN CALL multipleHaloexchange(repsToDo, dataSizeIter) END IF CALL MPI_Barrier(comm, ierr) finishTime = MPI_Wtime() totalTime = finishTime - startTime if (myMPIRank==0) then benchComplete = repTimeCheck(totalTime, repsToDo) end if call MPI_Bcast(benchComplete, 1, MPI_INTEGER, 0, comm, ierr) call MPI_Bcast(repsToDo, 1, MPI_INTEGER, 0, comm, ierr) END DO IF (myMPIRank == 0) THEN CALL setReportParams(dataSizeIter,repsToDo,totalTime) CALL printReport() END IF CALL freeData() dataSizeIter = dataSizeIter * 2 END DO END SUBROUTINE haloExchange
SUBROUTINE masteronlyHaloexchange(totalReps, dataSize) integer, intent(in) :: totalReps, dataSize integer :: repIter, i DO repIter = 1, totalReps DO i = 1, sizeofBuffer leftSendBuf(i) = globalIDarray(myThreadID) rightSendBuf(i) = globalIDarray(myThreadID) END DO CALL MPI_ISend(leftSendBuf, sizeofBuffer, MPI_INTEGER, & leftNeighbour, tag, comm, requestArray(1), ierr) CALL MPI_ISend(rightSendBuf, sizeofBuffer, MPI_INTEGER, & rightNeighbour, tag, comm, requestArray(2), ierr) CALL MPI_IRecv(leftRecvBuf, sizeofBuffer, MPI_INTEGER, & leftNeighbour, tag, comm, requestArray(3), ierr) CALL MPI_IRecv(rightRecvBuf, sizeofBuffer, MPI_INTEGER, & rightNeighbour, tag, comm, requestArray(4), ierr) CALL MPI_Waitall(4, requestArray, statusArray, ierr) DO i = 1, sizeofBuffer finalLeftBuf(i) = leftRecvBuf(i) finalRightBuf(i) = rightRecvBuf(i) END DO END DO END SUBROUTINE masteronlyHaloexchange
SUBROUTINE funnelledHaloexchange(totalReps, dataSize) integer, intent(in) :: totalReps, dataSize integer :: repIter, i DO repIter = 1, totalReps DO i = 1, sizeofBuffer leftSendBuf(i) = globalIDarray(myThreadID) rightSendBuf(i) = globalIDarray(myThreadID) END DO CALL MPI_ISend(leftSendBuf, sizeofBuffer, MPI_INTEGER, & leftNeighbour, tag, comm, requestArray(1), ierr) CALL MPI_ISend(rightSendBuf, sizeofBuffer, MPI_INTEGER, & rightNeighbour, tag, comm, requestArray(2), ierr) CALL MPI_IRecv(leftRecvBuf, sizeofBuffer, MPI_INTEGER, & leftNeighbour, tag, comm, requestArray(3), ierr) CALL MPI_IRecv(rightRecvBuf, sizeofBuffer, MPI_INTEGER, & rightNeighbour, tag, comm, requestArray(4), ierr) CALL MPI_Waitall(4, requestArray, statusArray, ierr) DO i = 1, sizeofBuffer finalLeftBuf(i) = leftRecvBuf(i) finalRightBuf(i) = rightRecvBuf(i) END DO END DO END SUBROUTINE funnelledHaloexchange
SUBROUTINE multipleHaloexchange(totalReps, dataSize) integer, intent(in) :: totalReps, dataSize integer :: repIter, i integer :: lBound, uBound DO repIter = 1, totalReps lBound = ((myThreadID-1) * dataSize) + 1 uBound = (myThreadID * dataSize) DO i = 1, sizeofBuffer leftSendBuf(i) = globalIDarray(myThreadID) rightSendBuf(i) = globalIDarray(myThreadID) END DO CALL MPI_Isend(leftSendBuf(lBound:uBound), dataSize, & MPI_INTEGER, leftNeighbour, myThreadID, comm, & requestArray(1), ierr) CALL MPI_Isend(rightSendBuf(lBound:uBound), dataSize, & MPI_INTEGER, rightNeighbour, myThreadID, comm, & requestArray(2), ierr) CALL MPI_IRecv(leftRecvBuf(lBound:uBound), dataSize, & MPI_INTEGER, leftNeighbour, myThreadID, comm, & requestArray(3), ierr) CALL MPI_IRecv(rightRecvBuf(lBound:uBound), dataSize, & MPI_INTEGER, rightNeighbour, myThreadID, comm, & requestArray(4), ierr) CALL MPI_Waitall(4, requestArray, statusArray, ierr) DO i = 1, sizeofBuffer finalLeftBuf(i) = leftRecvBuf(i) finalRightBuf(i) = rightRecvBuf(i) END DO END DO END SUBROUTINE multipleHaloexchange
SUBROUTINE allocateData(bufferSize) integer, intent(in) :: bufferSize allocate(leftSendBuf(bufferSize), leftRecvBuf(bufferSize)) allocate(rightSendBuf(bufferSize), rightRecvBuf(bufferSize)) allocate(finalLeftBuf(bufferSize), finalRightBuf(bufferSize)) END SUBROUTINE allocateData
SUBROUTINE freeData() deallocate(leftSendBuf, leftRecvBuf) deallocate(rightSendBuf, rightRecvBuf) deallocate(finalLeftBuf, finalRightBuf) END SUBROUTINE freeData
SUBROUTINE testHaloexchange(sizeofBuffer, dataSize) integer, intent(in) :: sizeofBuffer, dataSize integer :: i logical :: testFlag, reduceFlag testFlag = .true. allocate(testLeftBuf(sizeofBuffer),testRightBuf(sizeofBuffer)) DO i = 1, sizeofBuffer testLeftBuf(i) = (leftNeighbour * numThreads) + myThreadID testRightBuf(i) = (rightNeighbour * numThreads) + myThreadID END DO DO i = 1, sizeofBuffer IF (testLeftBuf(i) /= finalLeftBuf(i)) THEN testFlag = .false. END IF IF (testRightBuf(i) /= finalRightBuf(i)) THEN testFlag = .false. END IF END DO CALL MPI_Reduce(testFlag, reduceFlag, 1, MPI_LOGICAL, & MPI_LAND, 0, comm, ierr) IF (myMPIRank == 0) THEN CALL setTestOutcome(reduceFlag) END IF deallocate(testLeftBuf, testRightBuf) END SUBROUTINE testHaloexchange
SUBROUTINE multiPingPing(benchmarkType) integer, intent(in) :: benchmarkType integer :: dataSizeIter character (len = MPI_MAX_PROCESSOR_NAME) :: otherProcName logical :: balance pingNodeA = 0 pingNodeB = 1 balance = crossCommBalance(pingNodeA, pingNodeB) IF (balance .EQV. .false.) THEN IF (myMPIRank == 0) THEN CALL printBalanceError() END IF RETURN END IF CALL exchangeWorldRanks(pingNodeA, pingNodeB, otherPingRank) CALL sendProcName(pingNodeA, pingNodeB, otherProcName) CALL printMultiProcInfo(pingNodeA, otherPingRank, otherProcName) CALL MPI_Barrier(comm, ierr) IF(myMPIRank == 0) THEN CALL printBenchHeader() END IF repsToDo = defaultReps dataSizeIter = minDataSize DO WHILE (dataSizeIter <= maxDataSize) sizeofBuffer = dataSizeIter * numThreads CALL allocateData(sizeofBuffer) IF (benchmarkType == MASTERONLY) THEN CALL masteronlyMultiPingping(warmUpIters, dataSizeIter) ELSE IF (benchmarkType == FUNNELLED) THEN CALL funnelledMultiPingping(warmUpIters, dataSizeIter) ELSE IF (benchmarkType == MULTIPLE) THEN CALL multipleMultiPingping(warmUpIters, dataSizeIter) END IF CALL testMultiPingping(sizeofBuffer, dataSizeIter) benchComplete = .false. DO WHILE (benchComplete .NEQV. .true.) CALL MPI_Barrier(comm, ierr) startTime = MPI_Wtime() IF (benchmarkType == MASTERONLY) THEN CALL masteronlyMultiPingping(repsToDo, dataSizeIter) ELSE IF (benchmarkType == FUNNELLED) THEN CALL funnelledMultiPingping(repsToDo, dataSizeIter) ELSE IF (benchmarkType == MULTIPLE) THEN CALL multipleMultiPingping(repsToDo, dataSizeIter) END IF CALL MPI_Barrier(comm, ierr) finishTime = MPI_Wtime() totalTime = finishTime - startTime if (myMPIRank==0) then benchComplete = repTimeCheck(totalTime, repsToDo) end if call MPI_Bcast(benchComplete, 1, MPI_INTEGER, 0, comm, ierr) call MPI_Bcast(repsToDo, 1, MPI_INTEGER, 0, comm, ierr) END DO IF (myMPIRank == 0) THEN CALL setReportParams(dataSizeIter, repsToDo, totalTime) CALL printReport() END IF CALL freeData() dataSizeIter = dataSizeIter * 2 END DO END SUBROUTINE multiPingPing
SUBROUTINE masteronlyMultiPingping(totalReps, dataSize) integer, intent(in) :: totalReps, dataSize integer :: repIter, i integer :: destRank IF (crossCommRank == pingNodeA) THEN destRank = pingNodeB ELSE IF (crossCommRank == pingNodeB) THEN destRank = pingNodeA END IF DO repIter = 1, totalReps IF (crossCommRank == pingNodeA .or. & crossCommRank == pingNodeB) THEN DO i = 1, sizeofBuffer pingSendBuf(i) = globalIDarray(myThreadID) END DO CALL MPI_Isend(pingSendBuf, sizeofBuffer, MPI_INTEGER, & destRank, tag, crossComm, requestID, ierr) CALL MPI_Recv(pingRecvBuf, sizeofBuffer, MPI_INTEGER, & destRank, tag, crossComm, status, ierr) CALL MPI_Wait(requestID, status, ierr) DO i = 1, sizeofBuffer finalRecvBuf(i) = pingRecvBuf(i) END DO END IF END DO END SUBROUTINE masteronlyMultiPingping
SUBROUTINE funnelledMultiPingping(totalReps, dataSize) integer, intent(in) :: totalReps, dataSize integer :: repIter, i integer :: destRank IF (crossCommRank == pingNodeA) THEN destRank = pingNodeB ELSE IF (crossCommRank == pingNodeB) THEN destRank = pingNodeA END IF DO repIter = 1, totalReps IF (crossCommRank == pingNodeA .or. & crossCommRank == pingNodeB) THEN DO i = 1, sizeofBuffer pingSendBuf(i) = globalIDarray(myThreadID) END DO CALL MPI_Isend(pingSendBuf, sizeofBuffer, MPI_INTEGER, & destRank, tag, crossComm, requestID, ierr) CALL MPI_Recv(pingRecvBuf, sizeofBuffer, MPI_INTEGER, & destRank, tag, crossComm, status, ierr) CALL MPI_Wait(requestID, status, ierr) DO i = 1, sizeofBuffer finalRecvBuf(i) = pingRecvBuf(i) END DO END IF END DO END SUBROUTINE funnelledMultiPingping
SUBROUTINE multipleMultiPingping(totalReps, dataSize) integer, intent(in) :: totalReps, dataSize integer :: repIter, i integer :: destRank integer :: lBound, uBound IF (crossCommRank == pingNodeA) THEN destRank = pingNodeB ELSE IF (crossCommRank == pingNodeB) THEN destRank = pingNodeA END IF DO repIter = 1, totalReps IF (crossCommRank == pingNodeA .or. & crossCommRank == pingNodeB) THEN lBound = ((myThreadID-1) * dataSize) + 1 uBound = (myThreadID * dataSize) DO i = 1, sizeofBuffer pingSendBuf(i) = globalIDarray(myThreadID) END DO CALL MPI_Isend(pingSendBuf(lBound:uBound), dataSize, & MPI_INTEGER, destRank, myThreadID, crossComm, & requestID, ierr) CALL MPI_Recv(pingRecvBuf(lBound:uBound), dataSize, & MPI_INTEGER, destRank, myThreadID, crossComm, & status, ierr) CALL MPI_Wait(requestID, status, ierr) DO i = 1, sizeofBuffer finalRecvBuf(i) = pingRecvBuf(i) END DO END IF END DO END SUBROUTINE multipleMultiPingping
SUBROUTINE allocateData(sizeofBuffer) integer, intent(in) :: sizeofBuffer IF (crossCommRank == pingNodeA .or. & crossCommRank == pingNodeB) THEN allocate(pingSendBuf(sizeofBuffer)) allocate(pingRecvBuf(sizeofBuffer)) allocate(finalRecvBuf(sizeofBuffer)) END IF END SUBROUTINE allocateData
SUBROUTINE freeData() IF (crossCommRank == pingNodeA .or. & crossCommRank == pingNodeB) THEN deallocate(pingSendBuf, pingRecvBuf) deallocate(finalRecvBuf) END IF END SUBROUTINE freeData
SUBROUTINE testMultiPingping(sizeofBuffer, dataSize) integer, intent(in) :: sizeofBuffer, dataSize integer :: i logical :: testFlag, localTestFlag localTestFlag = .true. IF (crossCommRank == pingNodeA .or. & crossCommRank == pingNodeB) THEN allocate(testBuf(sizeofBuffer)) DO i = 1, sizeofBuffer testBuf(i) = (otherPingRank * numThreads) + myThreadID END DO DO i = 1, sizeofBuffer IF (testBuf(i) /= finalRecvBuf(i)) THEN localTestFlag = .false. END IF END DO deallocate(testBuf) END IF CALL MPI_Reduce(localTestFlag, testFlag, 1, MPI_LOGICAL, & MPI_LAND, 0, comm, ierr) IF (myMPIRank == 0) THEN CALL setTestOutcome(testFlag) END IF END SUBROUTINE testMultiPingping
SUBROUTINE multiPingPong(benchmarkType) integer, intent(in) :: benchmarkType integer :: dataSizeIter integer :: pongWorldRank character (len = MPI_MAX_PROCESSOR_NAME) :: pongProcName logical :: balance pingNode = 0 pongNode = 1 balance = crossCommBalance(pingNode, pongNode) IF (balance .EQV. .false.) THEN IF (myMPIRank == 0) THEN CALL printBalanceError() END IF RETURN END IF CALL exchangeWorldRanks(pingNode, pongNode, pongWorldRank) CALL sendProcName(pingNode, pongNode, pongProcName) CALL printMultiProcInfo(pingNode, pongWorldRank, pongProcName) CALL MPI_Barrier(comm, ierr) IF (myMPIRank == 0) THEN CALL printBenchHeader() END IF repsToDo = defaultReps dataSizeIter = minDataSize DO WHILE (dataSizeIter <= maxDataSize) sizeofBuffer = dataSizeIter * numThreads CALL allocateData(sizeofBuffer) IF (benchmarkType == MASTERONLY) THEN CALL masteronlyMultiPingpong(warmUpIters, dataSizeIter) ELSE IF (benchmarkType == FUNNELLED) THEN CALL funnelledMultiPingpong(warmUpIters, dataSizeIter) ELSE IF (benchmarkType == MULTIPLE) THEN CALL multipleMultiPingpong(warmUpIters, dataSizeIter) END IF CALL testMultiPingpong(sizeofBuffer, dataSizeIter) benchComplete = .false. DO WHILE (benchComplete .NEQV. .true.) CALL MPI_Barrier(comm, ierr) startTime = MPI_Wtime() IF (benchmarkType == MASTERONLY) THEN CALL masteronlyMultiPingpong(repsToDo, dataSizeIter) ELSE IF (benchmarkType == FUNNELLED) THEN CALL funnelledMultiPingpong(repsToDo, dataSizeIter) ELSE IF (benchmarkType == MULTIPLE) THEN CALL multipleMultiPingpong(repsToDo, dataSizeIter) END IF CALL MPI_Barrier(comm, ierr) finishTime = MPI_Wtime() totalTime = finishTime - startTime if (myMPIRank==0) then benchComplete = repTimeCheck(totalTime, repsToDo) end if call MPI_Bcast(benchComplete, 1, MPI_INTEGER, 0, comm, ierr) call MPI_Bcast(repsToDo, 1, MPI_INTEGER, 0, comm, ierr) END DO IF (myMPIRank == 0) THEN CALL setReportParams(dataSizeIter, repsToDo, totalTime) CALL printReport() END IF CALL freeData() dataSizeIter = dataSizeIter * 2 END DO END SUBROUTINE multiPingPong
SUBROUTINE masteronlyMultiPingpong(totalReps, dataSize) integer, intent(in) :: totalReps, dataSize integer :: repIter, i DO repIter = 1, totalReps IF (crossCommRank == pingNode) THEN DO i = 1, sizeofBuffer pingSendBuf(i) = globalIDarray(myThreadID) END DO CALL MPI_Send(pingSendBuf, sizeofBuffer, MPI_INTEGER,& pongNode, tag, crossComm, ierr) CALL MPI_Recv(pongRecvBuf, sizeofBuffer, MPI_INTEGER,& pongNode, tag, crossComm, status, ierr) DO i = 1, sizeofBuffer finalRecvBuf(i) = pongRecvBuf(i) END DO ELSEIF (crossCommRank == pongNode) THEN CALL MPI_Recv(pingRecvBuf, sizeofBuffer, MPI_INTEGER,& pingNode, tag, crossComm, status, ierr) DO i = 1,sizeofBuffer pongSendBuf(i) = pingRecvBuf(i) END DO CALL MPI_Send(pongSendBuf, sizeofBuffer, MPI_INTEGER, & pingNode, tag, crossComm, ierr) END IF END DO END SUBROUTINE masteronlyMultiPingpong
SUBROUTINE funnelledMultiPingpong(totalReps, dataSize) integer, intent(in) :: totalReps, dataSize integer :: repIter, i DO repIter = 1, totalReps IF (crossCommRank == pingNode) THEN DO i = 1, sizeofBuffer pingSendBuf(i) = globalIDarray(myThreadID) END DO CALL MPI_Send(pingSendBuf, sizeofBuffer, MPI_INTEGER,& pongNode, tag, crossComm, ierr) CALL MPI_Recv(pongRecvBuf, sizeofBuffer, MPI_INTEGER,& pongNode, tag, crossComm, status, ierr) DO i = 1, sizeofBuffer finalRecvBuf(i) = pongRecvBuf(i) END DO ELSE IF (crossCommRank == pongNode) THEN CALL MPI_Recv(pingRecvBuf, sizeofBuffer, MPI_INTEGER,& pingNode, tag, crossComm, status, ierr) DO i = 1, sizeofBuffer pongSendBuf(i) = pingRecvBuf(i) END DO CALL MPI_Send(pongSendBuf, sizeofBuffer, MPI_INTEGER,& pingNode, tag, crossComm, ierr) END IF END DO END SUBROUTINE funnelledMultiPingpong
SUBROUTINE multipleMultiPingpong(totalReps, dataSize) integer, intent(in) :: totalReps, dataSize integer :: repIter, i integer :: lBound, uBound DO repIter = 1, totalReps IF (crossCommRank == pingNode) THEN lBound = ((myThreadID-1)* dataSize) + 1 uBound = (myThreadID * dataSize) DO i = 1, sizeofBuffer pingSendBuf(i) = globalIDarray(myThreadID) END DO CALL MPI_Send(pingSendBuf(lBound:uBound), dataSize,& MPI_INTEGER, pongNode, myThreadID, crossComm, ierr) CALL MPI_Recv(pongRecvBuf(lBound:uBound), dataSize,& MPI_INTEGER, pongNode, myThreadID, crossComm, & status, ierr) DO i = 1, sizeofBuffer finalRecvBuf(i) = pongRecvBuf(i) END DO ELSEIF (crossCommRank == pongNode) THEN lBound = ((myThreadID - 1) * dataSize) + 1 uBound = (myThreadID * dataSize) CALL MPI_Recv(pingRecvBuf(lBound:uBound), dataSize, & MPI_INTEGER, pingNode, myThreadID, crossComm, & status, ierr) DO i = 1, sizeofBuffer pongSendBuf(i) = pingRecvBuf(i) END DO CALL MPI_Send(pongSendBuf(lBound:uBound), dataSize,& MPI_INTEGER, pingNode, myThreadID, & crossComm, ierr) END IF END DO END SUBROUTINE multipleMultiPingpong
SUBROUTINE allocateData(sizeofBuffer) integer, intent(in) :: sizeofBuffer IF (crossCommRank == pingNode) THEN allocate(pingSendBuf(sizeofBuffer)) allocate(pongRecvBuf(sizeofBuffer)) allocate(finalRecvBuf(sizeofBuffer)) ELSE IF (crossCommRank == pongNode) THEN allocate(pingRecvBuf(sizeofBuffer)) allocate(pongSendBuf(sizeofBuffer)) END IF END SUBROUTINE allocateData
SUBROUTINE freeData() IF (crossCommRank == pingNode) THEN deallocate(pingSendBuf) deallocate(pongRecvBuf) deallocate(finalRecvBuf) ELSE IF (crossCommRank == pongNode) THEN deallocate(pingRecvBuf) deallocate(pongSendBuf) END IF END SUBROUTINE freeData
SUBROUTINE testMultiPingpong(sizeofBuffer, dataSize) integer, intent(in) :: sizeofBuffer, dataSize integer :: i logical :: testFlag, localTestFlag localTestFlag = .true. IF (crossCommRank == pingNode) THEN allocate(testBuf(sizeofBuffer)) DO i = 1,sizeofBuffer testBuf(i) = globalIDarray(myThreadID) END DO DO i = 1, sizeofBuffer IF (testBuf(i) /= finalRecvBuf(i)) THEN localTestFlag = .false. END IF END DO deallocate(testBuf) END IF CALL MPI_Reduce(localTestFlag, testFlag, 1, MPI_LOGICAL, & MPI_LAND, 0, comm, ierr) IF (myMPIRank == 0) THEN CALL setTestOutcome(testFlag) END IF END SUBROUTINE testMultiPingpong
SUBROUTINE pingPing(benchmarkType) integer, intent(in) :: benchmarkType integer :: dataSizeIter logical :: sameNode pingRankA = PPRanks(1) pingRankB = PPRanks(2) sameNode = compareProcNames(pingRankA, pingRankB) IF (myMPIRank == 0) THEN CALL printNodeReport(sameNode,pingRankA,pingRankB) CALL printBenchHeader() END IF repsToDo = defaultReps dataSizeIter = minDataSize DO WHILE (dataSizeIter <= maxDataSize) sizeofBuffer = dataSizeIter * numThreads CALL allocateData(sizeofBuffer) IF (benchmarkType == MASTERONLY) THEN CALL masteronlyPingping(warmUpIters, dataSizeIter) ELSE IF (benchmarkType == FUNNELLED) THEN CALL funnelledPingping(warmUpIters, dataSizeIter) ELSE IF (benchmarkType == MULTIPLE) THEN CALL multiplePingping(warmUpIters, dataSizeIter) END IF CALL testPingping(sizeofBuffer, dataSizeIter) benchComplete = .false. DO WHILE (benchComplete .NEQV. .true.) CALL MPI_Barrier(comm, ierr) startTime = MPI_Wtime() IF (benchmarkType == MASTERONLY) THEN CALL masteronlyPingping(repsToDo, dataSizeIter) ELSE IF (benchmarkType == FUNNELLED) THEN CALL funnelledPingping(repsToDo, dataSizeIter) ELSE IF (benchmarkType == MULTIPLE) THEN CALL multiplePingping(repsToDo, dataSizeIter) END IF CALL MPI_Barrier(comm, ierr) finishTime = MPI_Wtime() totalTime = finishTime - startTime if (myMPIRank==0) then benchComplete = repTimeCheck(totalTime, repsToDo) end if call MPI_Bcast(benchComplete, 1, MPI_INTEGER, 0, comm, ierr) call MPI_Bcast(repsToDo, 1, MPI_INTEGER, 0, comm, ierr) END DO IF (myMPIRank == 0) THEN CALL setReportParams(dataSizeIter,repsToDo,totalTime) CALL printReport() END IF CALL freeData() dataSizeIter = dataSizeIter * 2 END DO END SUBROUTINE pingPing
SUBROUTINE masteronlyPingping(totalReps, dataSize) integer, intent(in) :: totalReps, dataSize integer ::  repIter, i integer :: destRank IF (myMPIRank == pingRankA) THEN destRank = pingRankB ELSE IF (myMPIRank == pingRankB) THEN destRank = pingRankA END IF DO repIter = 1, totalReps IF(myMPIRank == pingRankA .or. myMPIRank == pingRankB) THEN DO i = 1, sizeofBuffer pingSendBuf(i) = globalIDarray(myThreadID) END DO CALL MPI_ISend(pingSendBuf, sizeofBuffer, MPI_INTEGER, & destRank, tag, comm, requestID, ierr) CALL MPI_Recv(pingRecvBuf, sizeofBuffer, MPI_INTEGER, & destRank, tag, comm, status, ierr) CALL MPI_Wait(requestID, status, ierr) DO i = 1, sizeofBuffer finalRecvBuf(i) = pingRecvBuf(i) END DO END IF END DO END SUBROUTINE masteronlyPingping
