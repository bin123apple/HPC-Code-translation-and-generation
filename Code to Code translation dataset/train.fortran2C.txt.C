int freeAlltoallData(){ free(alltoallSendBuf); free(alltoallRecvBuf); free(alltoallFinalBuf); return 0; }
int testAlltoall(int dataSize){ int sizeofBuffer, i, j; int dataForEachThread, startElem; int testFlag, reduceFlag; int *testBuf; testFlag = TRUE; sizeofBuffer = dataSize * numThreads * numMPIprocs * numThreads; testBuf = (int *) malloc(sizeofBuffer * sizeof(int)); dataForEachThread = dataSize * numThreads * numMPIprocs; #pragma omp parallel default(none) \ private(i,j,startElem) \ shared(testBuf,globalIDarray,sizeofBuffer,dataSize) \ shared(numThreads,numMPIprocs,dataForEachThread) { startElem = (myThreadID) * dataForEachThread; for (i=0; i<(numThreads * numMPIprocs); i++){ for (j=0; j<dataSize; j++){ testBuf[startElem + (i * dataSize) + j] = i; } } } for (i=0; i<sizeofBuffer; i++){ if (alltoallFinalBuf[i] != testBuf[i]){ testFlag = FALSE; } } MPI_Reduce(&testFlag, &reduceFlag, 1, MPI_INT, MPI_LAND, 0, comm); if (myMPIRank == 0){ setTestOutcome(reduceFlag); } free(testBuf); return 0; }
int barrierDriver(){ repsToDo = defaultReps; barrierKernel(warmUpIters); benchComplete = FALSE; while (benchComplete != TRUE){ MPI_Barrier(comm); startTime = MPI_Wtime(); barrierKernel(repsToDo); MPI_Barrier(comm); finishTime = MPI_Wtime(); totalTime = finishTime - startTime; * repetitions. */ if (myMPIRank==0){ benchComplete = repTimeCheck(totalTime, repsToDo); } MPI_Bcast(&benchComplete, 1, MPI_INT, 0, comm); MPI_Bcast(&repsToDo, 1, MPI_INT, 0, comm); } if (myMPIRank == 0){ setTestOutcome(TRUE); setReportParams(1,repsToDo,totalTime); printReport(); } return 0; }
int barrierKernel(int totalReps){ int repIter; #pragma omp parallel default(none) \ private(repIter) \ shared(totalReps,comm) { for (repIter=0; repIter<totalReps; repIter++){ #pragma omp barrier #pragma omp master { MPI_Barrier(comm); } } } return 0; }
int broadcast(){ int dataSizeIter, sizeofFinalBuf; repsToDo = defaultReps; dataSizeIter = minDataSize; while (dataSizeIter <= maxDataSize){ allocateBroadcastData(dataSizeIter); broadcastKernel(warmUpIters,dataSizeIter); sizeofFinalBuf = dataSizeIter * numThreads; testBroadcast(sizeofFinalBuf); benchComplete = FALSE; while (benchComplete != TRUE){ MPI_Barrier(comm); startTime = MPI_Wtime(); broadcastKernel(repsToDo, dataSizeIter); MPI_Barrier(comm); finishTime = MPI_Wtime(); totalTime = finishTime - startTime; if (myMPIRank==0){ benchComplete = repTimeCheck(totalTime, repsToDo); } MPI_Bcast(&benchComplete, 1, MPI_INT, 0, comm); MPI_Bcast(&repsToDo, 1, MPI_INT, 0, comm); } if (myMPIRank == 0){ setReportParams(dataSizeIter, repsToDo, totalTime); printReport(); } freeBroadcastData(); dataSizeIter = dataSizeIter * 2; } return 0; }
int broadcastKernel(int totalReps, int dataSize){ int repIter, i; int startPos; for (repIter=0; repIter<totalReps; repIter++){ if (myMPIRank == BROADCASTROOT){ for (i=0; i<dataSize; i++){ broadcastBuf[i] = BROADCASTNUM; } } MPI_Bcast(broadcastBuf, dataSize, MPI_INT, BROADCASTROOT, comm); #pragma omp parallel default(none) \ private(i,startPos) \ shared(dataSize,finalBroadcastBuf,broadcastBuf) { startPos = ((myThreadID) * dataSize); for (i=0; i<dataSize; i++){ finalBroadcastBuf[startPos + i] = broadcastBuf[i]; } } } return 0; }
int allocateBroadcastData(int bufferSize){ broadcastBuf = (int *)malloc(bufferSize * sizeof(int)); finalBroadcastBuf = (int *)malloc((bufferSize*numThreads)*sizeof(int)); return 0; }
int freeBroadcastData(){ free(broadcastBuf); free(finalBroadcastBuf); return 0; }
int testBroadcast(int bufferSize){ int i, testFlag, reduceFlag; testFlag = TRUE; for (i=0; i<bufferSize; i++){ if (finalBroadcastBuf[i] != BROADCASTNUM){ testFlag = FALSE; } } MPI_Reduce(&testFlag, &reduceFlag, 1, MPI_INT, MPI_LAND, 0, comm); if (myMPIRank == 0){ setTestOutcome(testFlag); } return 0; }
int reduction(int benchmarkType){ int dataSizeIter, sizeofBuf; repsToDo = defaultReps; dataSizeIter = minDataSize; while (dataSizeIter <= maxDataSize){ allocateReduceData(dataSizeIter); if (benchmarkType == REDUCE){ reduceKernel(warmUpIters, dataSizeIter); if (myMPIRank == 0){ testReduce(dataSizeIter, benchmarkType); } } else if (benchmarkType == ALLREDUCE){ sizeofBuf = dataSizeIter * numThreads; allReduceKernel(warmUpIters, dataSizeIter); testReduce(sizeofBuf, benchmarkType); } benchComplete = FALSE; while (benchComplete != TRUE){ MPI_Barrier(comm); startTime = MPI_Wtime(); if (benchmarkType == REDUCE){ reduceKernel(repsToDo, dataSizeIter); } else if (benchmarkType == ALLREDUCE){ allReduceKernel(repsToDo, dataSizeIter); } MPI_Barrier(comm); finishTime = MPI_Wtime(); totalTime = finishTime - startTime; if (myMPIRank==0){ benchComplete = repTimeCheck(totalTime, repsToDo); } MPI_Bcast(&benchComplete, 1, MPI_INT, 0, comm); MPI_Bcast(&repsToDo, 1, MPI_INT, 0, comm); } if (myMPIRank == 0){ setReportParams(dataSizeIter, repsToDo, totalTime); printReport(); } freeReduceData(); dataSizeIter = dataSizeIter * 2; } return 0; }
int reduceKernel(int totalReps, int dataSize){ int repIter, i, j; for (repIter=1; repIter<totalReps; repIter++){ #pragma omp parallel default(none) \ private(i,j) \ shared(tempBuf,globalIDarray,dataSize,numThreads) \ shared(localReduceBuf) { #pragma omp for schedule(static,dataSize) for(i=0; i<(numThreads * dataSize); i++){ tempBuf[i] = globalIDarray[myThreadID]; } #pragma omp for for(i=0; i<dataSize; i++){ localReduceBuf[i] = 0; for (j=0; j<numThreads; j++){ localReduceBuf[i] += tempBuf[(j*dataSize)+i]; } } } MPI_Reduce(localReduceBuf, globalReduceBuf, dataSize,\ MPI_INT, MPI_SUM, 0, comm); if (myMPIRank==0) { for (i=0; i<dataSize; i++){ finalReduceBuf[i] = globalReduceBuf[i]; } } } return 0; }
int allReduceKernel(int totalReps, int dataSize){ int repIter, i, j; int startPos; for (repIter=0; repIter<totalReps; repIter++){ #pragma omp parallel default(none) \ private(i,j) \ shared(tempBuf,globalIDarray,dataSize,numThreads) \ shared(localReduceBuf) { #pragma omp for schedule(static,dataSize) for(i=0; i<(numThreads * dataSize); i++){ tempBuf[i] = globalIDarray[myThreadID]; } #pragma omp for for(i=0; i<dataSize; i++){ localReduceBuf[i] = 0; for (j=0; j<numThreads; j++){ localReduceBuf[i] += tempBuf[(j*dataSize)+i]; } } } MPI_Allreduce(localReduceBuf, globalReduceBuf, \ dataSize, MPI_INTEGER, MPI_SUM, comm); #pragma omp parallel default(none) \ private(i,startPos) \ shared(dataSize,finalReduceBuf,globalReduceBuf) { startPos = (myThreadID * dataSize); for (i=0; i<dataSize; i++){ finalReduceBuf[startPos + i] = globalReduceBuf[i]; } } } return 0; }
int allocateReduceData(int bufferSize){ localReduceBuf = (int *) malloc(bufferSize * sizeof(int)); globalReduceBuf = (int *) malloc(bufferSize * sizeof(int)); tempBuf = (int *) malloc((bufferSize * numThreads) * sizeof(int)); finalReduceBuf = (int *) malloc((bufferSize * numThreads) * sizeof(int)); return 0; }
int freeReduceData(){ free(localReduceBuf); free(globalReduceBuf); free(tempBuf); free(finalReduceBuf); return 0; }
int testReduce(int bufferSize, int benchmarkType){ int i, testFlag, reduceFlag; int correctReduce, lastGlobalID; correctReduce = 0; testFlag = TRUE; lastGlobalID = (numMPIprocs * numThreads); for (i=0; i<lastGlobalID; i++){ correctReduce = correctReduce + i; } for (i=0; i<bufferSize; i++){ if (finalReduceBuf[i] != correctReduce){ testFlag = FALSE; } } if (benchmarkType == ALLREDUCE){ MPI_Reduce(&testFlag, &reduceFlag, 1, MPI_INT, MPI_LAND, 0, comm); if (myMPIRank == 0){ setTestOutcome(reduceFlag); } } else{ setTestOutcome(testFlag); } return 0; }
int scatterGather(int benchmarkType){ int dataSizeIter, bufferSize; repsToDo = defaultReps; dataSizeIter = minDataSize; /* initialise dataSizeIter */ while (dataSizeIter <= maxDataSize){ bufferSize = dataSizeIter * numThreads; if (benchmarkType == SCATTER){ allocateScatterGatherData(bufferSize, benchmarkType); scatterKernel(warmUpIters, dataSizeIter); testScatterGather(bufferSize, benchmarkType); } else if (benchmarkType == GATHER){ allocateScatterGatherData(bufferSize, benchmarkType); gatherKernel(warmUpIters, dataSizeIter); if (myMPIRank == GATHERROOT){ testScatterGather(bufferSize*numMPIprocs, benchmarkType); } } benchComplete = FALSE; while (benchComplete != TRUE){ MPI_Barrier(comm); startTime = MPI_Wtime(); if (benchmarkType == SCATTER){ scatterKernel(repsToDo, dataSizeIter); } else if (benchmarkType == GATHER){ gatherKernel(repsToDo, dataSizeIter); } MPI_Barrier(comm); finishTime = MPI_Wtime(); totalTime = finishTime - startTime; if (myMPIRank==0) { benchComplete = repTimeCheck(totalTime, repsToDo); } MPI_Bcast(&benchComplete, 1, MPI_INT, 0, comm); MPI_Bcast(&repsToDo, 1, MPI_INT, 0, comm); } if (myMPIRank == 0){ setReportParams(dataSizeIter, repsToDo, totalTime); printReport(); } freeScatterGatherData(benchmarkType); dataSizeIter = dataSizeIter * 2; } return 0; }
int scatterKernel(int totalReps, int dataSize){ int repIter, i; int totalSendBufElems, sendCount, recvCount; totalSendBufElems = numMPIprocs * numThreads * dataSize; sendCount = dataSize * numThreads; recvCount = sendCount; for (repIter=0; repIter<totalReps; repIter++){ if (myMPIRank == SCATTERROOT){ for (i=0; i<totalSendBufElems; i++){ scatterSendBuf[i] = SCATTERSTARTVAL + i; } } MPI_Scatter(scatterSendBuf, sendCount, MPI_INT, \ scatterRecvBuf, recvCount, MPI_INT, \ SCATTERROOT, comm); #pragma omp parallel for default(none)			\ private(i)						\ shared(dataSize,recvCount,finalBuf,scatterRecvBuf)	\ schedule(static,dataSize) for (i=0; i<recvCount; i++){ /* loop over all data in recv buffer */ finalBuf[i] = scatterRecvBuf[i]; } } /* End of loop over reps */ return 0; }
int gatherKernel(int totalReps, int dataSize){ int repIter,i; int totalRecvBufElems, sendCount, recvCount; int startVal; totalRecvBufElems = dataSize * numThreads * numMPIprocs; sendCount = dataSize * numThreads; recvCount = sendCount; startVal = (myMPIRank * sendCount) + GATHERSTARTVAL; for (repIter=0; repIter<totalReps; repIter++){ #pragma omp parallel for default(none)			\ private(i)						\ shared(gatherSendBuf,startVal,dataSize,sendCount)	\ schedule(static,dataSize) for (i=0; i<sendCount; i++){ gatherSendBuf[i] = startVal + i; } MPI_Gather(gatherSendBuf, sendCount, MPI_INT,\ gatherRecvBuf, recvCount, MPI_INT,\ GATHERROOT, comm); if (myMPIRank == GATHERROOT){ for (i=0; i<totalRecvBufElems; i++){ finalBuf[i] = gatherRecvBuf[i]; } } } return 0; }
int allocateScatterGatherData(int bufferSize, int benchmarkType){ if (benchmarkType == SCATTER){ if (myMPIRank == SCATTERROOT){ scatterSendBuf = (int *) malloc((bufferSize * numMPIprocs) * sizeof(int)); } scatterRecvBuf = (int *) malloc(bufferSize * sizeof(int)); finalBuf = (int *)malloc(bufferSize * sizeof(int)); } else if (benchmarkType == GATHER){ gatherSendBuf = (int *) malloc(bufferSize * sizeof(int)); if (myMPIRank == GATHERROOT){ gatherRecvBuf = (int *) malloc((bufferSize * numMPIprocs) * sizeof(int)); finalBuf = (int *) malloc((bufferSize * numMPIprocs) * sizeof(int)); } } return 0; }
int freeScatterGatherData(int benchmarkType){ if (benchmarkType == SCATTER){ if (myMPIRank == SCATTERROOT){ free(scatterSendBuf); } free(scatterRecvBuf); free(finalBuf); } else if (benchmarkType == GATHER){ free(gatherSendBuf); if (myMPIRank == GATHERROOT){ free(gatherRecvBuf); free(finalBuf); } } return 0; }
int testScatterGather(int sizeofBuffer, int benchmarkType){ int i, startVal; int testFlag, reduceFlag; int *testBuf; testFlag = TRUE; testBuf = (int *) malloc (sizeofBuffer * sizeof(int)); if (benchmarkType == SCATTER){ startVal = (myMPIRank * sizeofBuffer) + SCATTERSTARTVAL; } else if (benchmarkType == GATHER){ startVal = GATHERSTARTVAL; } for (i=0; i<sizeofBuffer; i++){ testBuf[i] = startVal + i; } for (i=0; i<sizeofBuffer; i++){ if (finalBuf[i] != testBuf[i]){ testFlag = FALSE; } } if (benchmarkType == SCATTER){ MPI_Reduce(&testFlag, &reduceFlag, 1, MPI_INT, MPI_LAND, 0, comm); if (myMPIRank == 0){ setTestOutcome(reduceFlag); } } else if (benchmarkType == GATHER){ setTestOutcome(testFlag); } free(testBuf); return 0; }
int main(int argc, char *argv[]){ int supportFlag; char name[MAXSTRING]; initParallelEnv(); if (myMPIRank == 0){ if (argc != 2){ printf("ERROR Reading input file from command line.\n"); printf("Usage: %s <filename>", argv[0] ); finaliseParallelEnv(); exit(-1); } else{ printHeader(numMPIprocs,numThreads,threadSupport); openFile(argv[1]); setupBenchmarkList(); } } readBenchmarkParams(); while (findBenchmarkNumber() != FINISHED){ switch (benchmarkNumber){ case 0: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED); strcpy(name,"Masteronly Pingpong"); setBenchName(name, benchmarkNumber, supportFlag); } pingPong(MASTERONLY); break; case 1: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED); strcpy(name,"Funnelled Pingpong"); setBenchName(name, benchmarkNumber, supportFlag); } pingPong(FUNNELLED); break; case 2: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_MULTIPLE); strcpy(name,"Multiple Pingpong"); setBenchName(name, benchmarkNumber, supportFlag); } pingPong(MULTIPLE); break; case 3: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED); strcpy(name,"Masteronly Pingping"); setBenchName(name, benchmarkNumber, supportFlag); } pingPing(MASTERONLY); break; case 4: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED); strcpy(name,"Funnelled Pingping"); setBenchName(name, benchmarkNumber, supportFlag); } pingPing(FUNNELLED); break; case 5: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_MULTIPLE); strcpy(name,"Multiple Pingping"); setBenchName(name, benchmarkNumber, supportFlag); } pingPing(MULTIPLE); break; case 6: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED); strcpy(name,"Masteronly Haloexchange"); setBenchName(name, benchmarkNumber, supportFlag); printBenchHeader(); } haloExchange(MASTERONLY); break; case 7: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED); strcpy(name,"Funnelled Haloexchange"); setBenchName(name, benchmarkNumber, supportFlag); printBenchHeader(); } haloExchange(FUNNELLED); break; case 8: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_MULTIPLE); strcpy(name,"Multiple Haloexchange"); setBenchName(name, benchmarkNumber, supportFlag); printBenchHeader(); } haloExchange(MULTIPLE); break; case 9: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED); strcpy(name,"Masteronly MultiPingpong"); setBenchName(name, benchmarkNumber, supportFlag); } multiPingPong(MASTERONLY); break; case 10: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED); strcpy(name,"Funnelled MultiPingpong"); setBenchName(name, benchmarkNumber, supportFlag); } multiPingPong(FUNNELLED); break; case 11: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_MULTIPLE); strcpy(name,"Multiple MultiPingpong"); setBenchName(name, benchmarkNumber, supportFlag); } multiPingPong(MULTIPLE); break; case 12: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED); strcpy(name,"Masteronly MultiPingping"); setBenchName(name, benchmarkNumber, supportFlag); } multiPingping(MASTERONLY); break; case 13: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED); strcpy(name,"Funnelled MultiPingping"); setBenchName(name, benchmarkNumber, supportFlag); } multiPingping(FUNNELLED); break; case 14: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_MULTIPLE); strcpy(name, "Multiple MultiPingping"); setBenchName(name, benchmarkNumber, supportFlag); } multiPingping(MULTIPLE); break; case 15: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED); strcpy(name,"Barrier"); setBenchName(name, benchmarkNumber, supportFlag); printBenchHeader(); } barrierDriver(); break; case 16: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED); strcpy(name,"Reduce"); setBenchName(name, benchmarkNumber, supportFlag); printBenchHeader(); } reduction(REDUCE); break; case 17: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED); strcpy(name,"All Reduce"); setBenchName(name, benchmarkNumber, supportFlag); printBenchHeader(); } reduction(ALLREDUCE); break; case 18: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED); strcpy(name,"Broadcast"); setBenchName(name, benchmarkNumber, supportFlag); printBenchHeader(); } broadcast(); break; case 19: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED); strcpy(name,"Scatter"); setBenchName(name, benchmarkNumber, supportFlag); printBenchHeader(); } scatterGather(SCATTER); break; case 20: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED); strcpy(name,"Gather"); setBenchName(name, benchmarkNumber, supportFlag); printBenchHeader(); } scatterGather(GATHER); break; case 21: if (myMPIRank == 0){ supportFlag = benchmarkSupport(MPI_THREAD_FUNNELED); strcpy(name,"All to All"); setBenchName(name, benchmarkNumber, supportFlag); printBenchHeader(); } alltoall(); break; } } finaliseParallelEnv(); if (myMPIRank == 0){ closeFile(); } }
int printHeader(){ char string[MAXSTRING]; threadSupportToString(benchReport.supportLevel, string); printf("----------------------------------------------\n"); printf("Mixed mode MPI/OpenMP benchmark suite v1.0\n"); printf("----------------------------------------------\n"); printf("Number of MPI processes = %d\n", benchReport.numMPIprocs); printf("Number of OpenMP threads = %d\n", benchReport.numThreads); printf("Thread support = %s\n", string); return 0; }
int setBenchName(char *name, int number, int support){ strcpy(benchReport.benchName,name); benchReport.benchNumber = number; benchReport.supported = support; printBenchName(); return 0; }
int setTestOutcome(int outcome){ if (outcome == TRUE){ strcpy(benchReport.testOutcome,"Pass"); } else if (outcome == FALSE){ strcpy(benchReport.testOutcome,"Fail"); } return 0; }
int setReportParams(int size, int reps, double time){ benchReport.dataSize = size; benchReport.numReps = reps; benchReport.benchTime = time; benchReport.timePerRep = time / reps; if (benchReport.benchNumber <= LAST_PT_PT_ID){ * dataSize x numThreads x sizeof(int) */ benchReport.bytes = size * benchReport.numThreads * sizeInteger; } else if (benchReport.benchNumber <= LASTMULTIPPID){ benchReport.bytes = size * benchReport.numThreads * sizeInteger * localCommSize; } else { benchReport.bytes = size * sizeInteger; } return 0; }
int printBenchName(){ printf("--------------------------------------------\n"); printf("# %s\n", benchReport.benchName); printf("--------------------------------------------\n"); if (benchReport.supported == FALSE){ printf("WARNING: Implementation does not support benchmark.\n"); } return 0; }
int printBenchHeader(){ printf(" Data Size     Msg Size (bytes)     No. Reps     "); printf("Time (sec)     Time/Rep (s)     Test\n"); printf("-----------   ------------------   ----------   "); printf("------------   --------------   ------\n"); return 0; }
int printNodeReport(int sameNode, int rankA, int rankB){ if (sameNode == TRUE){ printf("Intra node benchmark between process %d and process %d\n",rankA,rankB); } else if (sameNode == FALSE){ printf("Inter node benchmark between process %d and process %d\n",rankA,rankB); } return 0; }
int printMultiProcInfo(int printNode, int pairWorldRank, char *pairProcName){ if (crossCommRank == printNode){ printf("MPI process %d on %s ", myMPIRank,myProcName); printf("communicating with MPI process %d on %s\n",\ pairWorldRank,pairProcName); } return 0; }
int printReport(){ printf("d %d\t\t%d\t\t   %d\t\t%lf\t%lf\t%s\n", \ benchReport.dataSize, benchReport.bytes, benchReport.numReps, \ benchReport.benchTime, benchReport.timePerRep,benchReport.testOutcome); return 0; }
int printBalanceError(){ printf("\nERROR: Nodes selected for this benchmark do not "); printf("have same number of MPI processes per node."); printf("Skipping benchmark...\n"); return 0; }
int threadSupportToString(int threadSupport, char *string){ if (threadSupport == MPI_THREAD_SINGLE){ strcpy(string,"MPI_THREAD_SINGLE"); } else if (threadSupport == MPI_THREAD_FUNNELED){ strcpy(string,"MPI_THREAD_FUNNELED"); } else if (threadSupport == MPI_THREAD_SERIALIZED){ strcpy(string,"MPI_THREAD_SERIALIZED"); } else if (threadSupport == MPI_THREAD_MULTIPLE){ strcpy(string,"MPI_THREAD_MULTIPLE"); } return 0; }
int initParallelEnv(){ MPI_Init_thread(NULL, NULL, MPI_THREAD_MULTIPLE, &threadSupport); comm = MPI_COMM_WORLD; MPI_Comm_size(comm, &numMPIprocs); MPI_Comm_rank(comm, &myMPIRank); sizeInteger = sizeof(int); MPI_Get_processor_name(myProcName, &procNameLen); * across node boundaries. */ setupCommunicators(); #pragma omp parallel default(none) \ shared(numThreads,globalIDarray,myMPIRank) { numThreads = omp_get_num_threads(); myThreadID = omp_get_thread_num(); #pragma omp single { globalIDarray = (int *)malloc(numThreads * sizeof(int)); } globalIDarray[myThreadID] = (myMPIRank * numThreads) + myThreadID; } setParallelInfo(numMPIprocs,threadSupport,numThreads); return 0; }
int finaliseParallelEnv(){ MPI_Finalize(); free(globalIDarray); return 0; }
int findRank(int rankIn){ int CalcRank; if (rankIn < 0){ CalcRank = numMPIprocs + rankIn; } else{ CalcRank = rankIn; } if (CalcRank > (numMPIprocs-1)){ printf("Warning: Rank input greater than total process count.\n"); printf("Using Rank = %d ", numMPIprocs-1); CalcRank = numMPIprocs - 1; } else if(CalcRank < 0){ printf("Warning: MPI process offset greater than total process count.\n"); printf("Using Rank = 0 "); CalcRank = 0; } return CalcRank; }
int benchmarkSupport(int required){ int benchSupport; if (required <= threadSupport){ benchSupport = TRUE; } else { benchSupport = FALSE; } return benchSupport; }
int compareProcNames(int rankA, int rankB){ int sameNode; char recvProcName[MPI_MAX_PROCESSOR_NAME]; if (myMPIRank == rankB){ MPI_Send(myProcName, MPI_MAX_PROCESSOR_NAME, MPI_CHAR, rankA, TAG, comm); } else if (myMPIRank == rankA){ MPI_Recv(recvProcName, MPI_MAX_PROCESSOR_NAME, MPI_CHAR, rankB, TAG, comm, &status); if (strcmp(myProcName,recvProcName) == 0){ sameNode = TRUE; } else{ sameNode = FALSE; } } MPI_Bcast(&sameNode, 1, MPI_INT, rankA, comm); return sameNode; }
int setupCommunicators(){ int procHash; procHash = procNameToHash(); * local communicator. */ MPI_Comm_split(comm, procHash, 0, &localComm); MPI_Comm_rank(localComm, &localCommRank); MPI_Comm_size(localComm, &localCommSize); MPI_Comm_split(comm, localCommRank, 0, &crossComm); MPI_Comm_rank(crossComm, &crossCommRank); return 0; }
int procNameToHash(){ int procHash,i; procHash = 0; for (i=0; i<procNameLen; i++){ procHash = (7 * procHash) + (int)(myProcName[i]); } return procHash; }
int exchangeWorldRanks(int nodeA, int nodeB, int *otherWorldRank){ int destRank; if (crossCommRank == nodeA){ destRank = nodeB; } else if (crossCommRank == nodeB){ destRank = nodeA; } if (crossCommRank == nodeA || crossCommRank == nodeB){ MPI_Isend(&myMPIRank, 1, MPI_INT, destRank, TAG, crossComm, &requestID); MPI_Recv(otherWorldRank, 1, MPI_INT, destRank, TAG, crossComm, &status); MPI_Wait(&requestID, &status); } return 0; }
int sendProcName(int destNode, int srcNode, char *destProcName){ if (crossCommRank == srcNode){ MPI_Send(myProcName, MPI_MAX_PROCESSOR_NAME, MPI_CHAR, \ destNode, TAG, crossComm); } else if (crossCommRank == destNode){ MPI_Recv(destProcName, MPI_MAX_PROCESSOR_NAME, MPI_CHAR, \ srcNode, TAG, crossComm, &status); } }
int crossCommBalance(int nodeA, int nodeB){ int localCommSize, otherLocalCommSize; int crossCommBalance; MPI_Comm_size(localComm, &localCommSize); if ((crossCommRank == nodeB) && (localCommRank == 0)){ MPI_Send(&localCommSize, 1, MPI_INT, nodeA, TAG, crossComm); } else if ((crossCommRank == nodeA) && (localCommRank == 0)){ MPI_Recv(&otherLocalCommSize, 1, MPI_INT, nodeB, TAG, \ crossComm, &status); if (localCommSize == otherLocalCommSize){ crossCommBalance = TRUE; } else{ crossCommBalance = FALSE; } if (myMPIRank != 0){ MPI_Send(&crossCommBalance, 1, MPI_INT, 0, TAG, comm); } } if (myMPIRank == 0){ if ((crossCommRank != nodeA) && (localCommRank != 0)){ MPI_Recv(&crossCommRank, 1, MPI_INT, MPI_ANY_SOURCE, \ TAG, comm, &status); } } MPI_Bcast(&crossCommBalance, 1, MPI_INT, 0, comm); return crossCommBalance; }
int haloExchange(int benchmarkType){ int dataSizeIter; findNeighbours(); repsToDo = defaultReps; dataSizeIter = minDataSize; /* Initialise dataSizeIter */ while (dataSizeIter <= maxDataSize){ sizeofBuffer = dataSizeIter * numThreads; allocateHaloexchangeData(sizeofBuffer); if (benchmarkType == MASTERONLY){ masteronlyHaloexchange(warmUpIters, dataSizeIter); } else if (benchmarkType == FUNNELLED){ funnelledHaloexchange(warmUpIters, dataSizeIter); } else if (benchmarkType == MULTIPLE){ multipleHaloexchange(warmUpIters, dataSizeIter); } testHaloexchange(sizeofBuffer, dataSizeIter); benchComplete = FALSE; while (benchComplete != TRUE){ MPI_Barrier(comm); startTime = MPI_Wtime(); if (benchmarkType == MASTERONLY){ masteronlyHaloexchange(repsToDo, dataSizeIter); } else if (benchmarkType == FUNNELLED){ funnelledHaloexchange(repsToDo, dataSizeIter); } else if (benchmarkType == MULTIPLE){ multipleHaloexchange(repsToDo, dataSizeIter); } MPI_Barrier(comm); finishTime = MPI_Wtime(); totalTime = finishTime - startTime; if (myMPIRank==0){ benchComplete = repTimeCheck(totalTime, repsToDo); } MPI_Bcast(&benchComplete, 1, MPI_INT, 0, comm); MPI_Bcast(&repsToDo, 1, MPI_INT, 0, comm); } if (myMPIRank == 0 ){ setReportParams(dataSizeIter, repsToDo, totalTime); printReport(); } freeHaloexchangeData(); dataSizeIter = dataSizeIter * 2; } return 0; }
int masteronlyHaloexchange(int totalReps, int dataSize){ int repIter, i; for (repIter=0; repIter<totalReps; repIter++){ * and leftSendBuf using a parallel for directive. */ #pragma omp parallel for default(none) \ private(i) \ shared(leftSendBuf,rightSendBuf,dataSize) \ shared(sizeofBuffer,globalIDarray) \ schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ leftSendBuf[i] = globalIDarray[myThreadID]; rightSendBuf[i] = globalIDarray[myThreadID]; } * rightNeighbour using non-blocking send... */ MPI_Isend(leftSendBuf, sizeofBuffer, MPI_INT, leftNeighbour, \ TAG, commCart, &requestArray[0]); MPI_Isend(rightSendBuf, sizeofBuffer, MPI_INT, rightNeighbour, \ TAG, commCart, &requestArray[1]); MPI_Irecv(leftRecvBuf, sizeofBuffer, MPI_INT, leftNeighbour, \ TAG, commCart, &requestArray[2]); MPI_Irecv(rightRecvBuf, sizeofBuffer, MPI_INT, rightNeighbour, \ TAG, commCart, &requestArray[3]); MPI_Waitall(4, requestArray, statusArray); #pragma omp parallel for default(none) \ private(i) \ shared(leftRecvBuf,rightRecvBuf,dataSize,sizeofBuffer) \ shared(finalLeftBuf,finalRightBuf) \ schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ finalLeftBuf[i] = leftRecvBuf[i]; finalRightBuf[i] = rightRecvBuf[i]; } } return 0; }
int funnelledHaloexchange(int totalReps, int dataSize){ int repIter, i; #pragma omp parallel default(none) \ private(i,repIter) \ shared(dataSize,sizeofBuffer,leftSendBuf,rightSendBuf) \ shared(rightRecvBuf,leftRecvBuf,finalLeftBuf,finalRightBuf) \ shared(globalIDarray,commCart,totalReps,requestArray,statusArray) \ shared(leftNeighbour,rightNeighbour) { for (repIter=0; repIter<totalReps; repIter++){ #pragma omp for schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ leftSendBuf[i] = globalIDarray[myThreadID]; rightSendBuf[i] = globalIDarray[myThreadID]; } #pragma omp master { MPI_Isend(leftSendBuf, sizeofBuffer, MPI_INT, leftNeighbour, \ TAG, commCart, &requestArray[0]); MPI_Isend(rightSendBuf, sizeofBuffer, MPI_INT, rightNeighbour, \ TAG, commCart, &requestArray[1]); MPI_Irecv(leftRecvBuf, sizeofBuffer, MPI_INT, leftNeighbour, \ TAG, commCart, &requestArray[2]); MPI_Irecv(rightRecvBuf, sizeofBuffer, MPI_INT, rightNeighbour, \ TAG, commCart, &requestArray[3]); MPI_Waitall(4, requestArray, statusArray); } #pragma omp barrier #pragma omp for schedule(static,dataSize) for(i=0; i<sizeofBuffer; i++){ finalLeftBuf[i] = leftRecvBuf[i]; finalRightBuf[i] = rightRecvBuf[i]; } } } return 0; }
int multipleHaloexchange(int totalReps, int dataSize){ int repIter, i; int lBound; #pragma omp parallel default(none) \ private(i,requestArray,statusArray,lBound,repIter) \ shared(dataSize,sizeofBuffer,leftSendBuf,rightSendBuf) \ shared(rightRecvBuf,leftRecvBuf,finalLeftBuf,finalRightBuf) \ shared(leftNeighbour,rightNeighbour,globalIDarray,commCart,totalReps) { for (repIter=0; repIter<totalReps; repIter++){ lBound = (myThreadID * dataSize); #pragma omp for nowait schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ leftSendBuf[i] = globalIDarray[myThreadID]; rightSendBuf[i] = globalIDarray[myThreadID]; } MPI_Isend(&leftSendBuf[lBound], dataSize, MPI_INT, leftNeighbour, \ myThreadID, commCart, &requestArray[0]); MPI_Isend(&rightSendBuf[lBound], dataSize, MPI_INT, rightNeighbour, \ myThreadID, commCart, &requestArray[1]); MPI_Irecv(&leftRecvBuf[lBound], dataSize, MPI_INT, leftNeighbour, \ myThreadID, commCart, &requestArray[2]); MPI_Irecv(&rightRecvBuf[lBound], dataSize, MPI_INT, rightNeighbour, \ myThreadID, commCart, &requestArray[3]); MPI_Waitall(4, requestArray, statusArray); #pragma omp for nowait schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ finalLeftBuf[i] = leftRecvBuf[i]; finalRightBuf[i] = rightRecvBuf[i]; } } } return 0; }
int allocateHaloexchangeData(int sizeofBuffer){ leftSendBuf = (int *)malloc(sizeofBuffer * sizeof(int)); leftRecvBuf = (int *)malloc(sizeofBuffer * sizeof(int)); rightSendBuf = (int *)malloc(sizeofBuffer * sizeof(int)); rightRecvBuf = (int *)malloc(sizeofBuffer * sizeof(int)); finalLeftBuf = (int *)malloc(sizeofBuffer * sizeof(int)); finalRightBuf = (int *)malloc(sizeofBuffer * sizeof(int)); return 0; }
int freeHaloexchangeData(){ free(leftSendBuf); free(leftRecvBuf); free(rightSendBuf); free(rightRecvBuf); free(finalLeftBuf); free(finalRightBuf); return 0; }
int testHaloexchange(int sizeofBuffer, int dataSize){ int i; int testFlag, reduceFlag; int *testLeftBuf, *testRightBuf; testFlag = TRUE; testLeftBuf = (int *)malloc(sizeofBuffer * sizeof(int)); testRightBuf = (int *)malloc(sizeofBuffer * sizeof(int)); #pragma omp parallel for default(none) \ private(i) \ shared(leftNeighbour,rightNeighbour,numThreads) \ shared(dataSize,sizeofBuffer,testLeftBuf,testRightBuf) \ schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ testLeftBuf[i] = (leftNeighbour * numThreads) + myThreadID; testRightBuf[i] = (rightNeighbour * numThreads) + myThreadID; } for (i=0; i<sizeofBuffer; i++){ if (testLeftBuf[i] != finalLeftBuf[i]){ testFlag = FALSE; } if (testRightBuf[i] != finalRightBuf[i]){ testFlag = FALSE; } } MPI_Reduce(&testFlag, &reduceFlag, 1, MPI_INT, MPI_LAND, 0, comm); if (myMPIRank == 0){ setTestOutcome(reduceFlag); } free(testLeftBuf); free(testRightBuf); return 0; }
int multiPingping(int benchmarkType){ int dataSizeIter; char otherProcName[MPI_MAX_PROCESSOR_NAME]; int balance; pingNodeA = 0; pingNodeB = 1; balance = crossCommBalance(pingNodeA, pingNodeB); if (balance == FALSE){ if (myMPIRank == 0){ printBalanceError(); } return 1; } exchangeWorldRanks(pingNodeA, pingNodeB, &otherPingRank); sendProcName(pingNodeA, pingNodeB, otherProcName); printMultiProcInfo(pingNodeA, otherPingRank, otherProcName); MPI_Barrier(comm); if (myMPIRank == 0){ printBenchHeader(); } repsToDo = defaultReps; dataSizeIter = minDataSize; while (dataSizeIter <= maxDataSize){ sizeofBuffer = dataSizeIter * numThreads; allocateMultiPingpingData(sizeofBuffer); if (benchmarkType == MASTERONLY){ masteronlyMultiPingping(warmUpIters, dataSizeIter); } else if (benchmarkType == FUNNELLED){ funnelledMultiPingping(warmUpIters, dataSizeIter); } else if (benchmarkType == MULTIPLE){ multipleMultiPingping(warmUpIters, dataSizeIter); } testMultiPingping(sizeofBuffer, dataSizeIter); benchComplete = FALSE; while (benchComplete != TRUE){ MPI_Barrier(comm); startTime = MPI_Wtime(); if (benchmarkType == MASTERONLY){ masteronlyMultiPingping(repsToDo, dataSizeIter); } else if (benchmarkType == FUNNELLED){ funnelledMultiPingping(repsToDo, dataSizeIter); } else if (benchmarkType == MULTIPLE){ multipleMultiPingping(repsToDo, dataSizeIter); } MPI_Barrier(comm); finishTime = MPI_Wtime(); totalTime = finishTime - startTime; if (myMPIRank==0){ benchComplete = repTimeCheck(totalTime, repsToDo); } MPI_Bcast(&benchComplete, 1, MPI_INT, 0, comm); MPI_Bcast(&repsToDo, 1, MPI_INT, 0, comm); } /* End of loop to check if benchComplete is true */ if (myMPIRank == 0){ setReportParams(dataSizeIter, repsToDo, totalTime); printReport(); } freeMultiPingpingData(); dataSizeIter = dataSizeIter * 2; } return 0; }
int masteronlyMultiPingping(int totalReps, int dataSize){ int repIter, i; int destRank; if (crossCommRank == pingNodeA){ destRank = pingNodeB; } else if (crossCommRank == pingNodeB){ destRank = pingNodeA; } for (repIter=1; repIter<=totalReps; repIter++){ if ((crossCommRank == pingNodeA) || (crossCommRank == pingNodeB) ){ #pragma omp parallel for default(none)				\ private(i)							\ shared(pingSendBuf,dataSize,sizeofBuffer,globalIDarray)	\ schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ pingSendBuf[i] = globalIDarray[myThreadID]; } MPI_Isend(pingSendBuf, sizeofBuffer, MPI_INT, destRank, TAG,\ crossComm, &requestID); MPI_Recv(pingRecvBuf, sizeofBuffer, MPI_INT, destRank, TAG, \ crossComm, &status); MPI_Wait(&requestID, &status); #pragma omp parallel for default(none)				\ private(i)							\ shared(finalRecvBuf,dataSize,sizeofBuffer,pingRecvBuf)	\ schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ finalRecvBuf[i] = pingRecvBuf[i]; } } } /* End repetitions loop */ return 0; }
int funnelledMultiPingping(int totalReps, int dataSize){ int repIter, i; int destRank; if (crossCommRank == pingNodeA){ destRank = pingNodeB; } else if (crossCommRank == pingNodeB){ destRank = pingNodeA; } #pragma omp parallel default(none)				\ private(i,repIter)						\ shared(dataSize,sizeofBuffer,pingSendBuf,globalIDarray)	\ shared(pingRecvBuf,finalRecvBuf,status,requestID,destRank)	\ shared(crossComm,crossCommRank,pingNodeA,pingNodeB,totalReps) { for (repIter = 1; repIter <= totalReps; repIter++){ if (crossCommRank == pingNodeA || crossCommRank == pingNodeB){ #pragma omp for schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ pingSendBuf[i] = globalIDarray[myThreadID]; } #pragma omp master { MPI_Isend(pingSendBuf, sizeofBuffer, MPI_INT, \ destRank, TAG, crossComm, &requestID); MPI_Recv(pingRecvBuf, sizeofBuffer, MPI_INT, \ destRank, TAG, crossComm, &status); MPI_Wait(&requestID, &status); } #pragma omp barrier #pragma omp for schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ finalRecvBuf[i] = pingRecvBuf[i]; } } } /* End repetitions loop */ } /* End parallel region */ return 0; }
int multipleMultiPingping(int totalReps, int dataSize){ int repIter, i; int destRank; int lBound; if (crossCommRank == pingNodeA){ destRank = pingNodeB; } else if (crossCommRank == pingNodeB){ destRank = pingNodeA; } #pragma omp parallel default(none)				\ private(i,repIter,lBound,requestID,status)			\ shared(dataSize,sizeofBuffer,pingSendBuf,globalIDarray)	\ shared(pingRecvBuf,finalRecvBuf,destRank,crossComm)		\ shared(crossCommRank,pingNodeA,pingNodeB,totalReps) { for (repIter = 1; repIter <= totalReps; repIter++){ if (crossCommRank == pingNodeA || crossCommRank == pingNodeB){ lBound = (myThreadID * dataSize); #pragma omp for nowait schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ pingSendBuf[i] = globalIDarray[myThreadID]; } MPI_Isend(&pingSendBuf[lBound], dataSize, MPI_INT, \ destRank, myThreadID, crossComm, &requestID); MPI_Recv(&pingRecvBuf[lBound], dataSize, MPI_INT, destRank, \ myThreadID, crossComm, &status); MPI_Wait(&requestID, &status); #pragma omp for nowait schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ finalRecvBuf[i] = pingRecvBuf[i]; } } } /* End repetitions loop */ } return 0; }
int allocateMultiPingpingData(int sizeofBuffer){ if (crossCommRank == pingNodeA || crossCommRank == pingNodeB){ pingSendBuf = (int *)malloc(sizeof(int) * sizeofBuffer); pingRecvBuf = (int *)malloc(sizeof(int) * sizeofBuffer); finalRecvBuf = (int *)malloc(sizeof(int) * sizeofBuffer); } return 0; }
int freeMultiPingpingData(){ if (crossCommRank == pingNodeA || crossCommRank == pingNodeB){ free(pingSendBuf); free(pingRecvBuf); free(finalRecvBuf); } return 0; }
int testMultiPingping(int sizeofBuffer, int dataSize){ int i; int testFlag, localTestFlag; localTestFlag = TRUE; if (crossCommRank == pingNodeA || crossCommRank == pingNodeB) { testBuf = (int *)malloc(sizeof(int) * sizeofBuffer); #pragma omp parallel for default(none)					\ private(i)								\ shared(otherPingRank,numThreads,dataSize,sizeofBuffer,testBuf)	\ schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ testBuf[i] = (otherPingRank * numThreads) + myThreadID; } for (i=0; i<sizeofBuffer; i++){ if (testBuf[i] != finalRecvBuf[i]){ localTestFlag = FALSE; } } free(testBuf); } MPI_Reduce(&localTestFlag, &testFlag, 1, MPI_INT, MPI_LAND, 0, comm); if (myMPIRank == 0){ setTestOutcome(testFlag); } return 0; }
int multiPingPong(int benchmarkType){ int dataSizeIter; int pongWorldRank; char pongProcName[MPI_MAX_PROCESSOR_NAME]; int balance; pingNode = 0; pongNode = 1; balance = crossCommBalance(pingNode, pongNode); if (balance == FALSE){ if (myMPIRank == 0){ printBalanceError(); } return 1; } exchangeWorldRanks(pingNode, pongNode, &pongWorldRank); sendProcName(pingNode, pongNode, pongProcName); printMultiProcInfo(pingNode, pongWorldRank, pongProcName); MPI_Barrier(comm); if (myMPIRank == 0){ printBenchHeader(); } repsToDo = defaultReps; dataSizeIter = minDataSize; /* initialise dataSizeIter to minDataSize */ while (dataSizeIter <= maxDataSize){ sizeofBuffer = dataSizeIter * numThreads; allocateMultiPingpongData(sizeofBuffer); if (benchmarkType == MASTERONLY){ masteronlyMultiPingpong(warmUpIters, dataSizeIter); } else if (benchmarkType == FUNNELLED){ funnelledMultiPingpong(warmUpIters, dataSizeIter); } else if (benchmarkType == MULTIPLE){ multipleMultiPingpong(warmUpIters, dataSizeIter); } testMultiPingpong(sizeofBuffer, dataSizeIter); benchComplete = FALSE; while (benchComplete != TRUE){ Then start the timer. */ MPI_Barrier(comm); startTime = MPI_Wtime(); if (benchmarkType == MASTERONLY){ masteronlyMultiPingpong(repsToDo, dataSizeIter); } else if (benchmarkType == FUNNELLED){ funnelledMultiPingpong(repsToDo, dataSizeIter); } else if (benchmarkType == MULTIPLE){ multipleMultiPingpong(repsToDo, dataSizeIter); } * for more accurate timing. */ MPI_Barrier(comm); finishTime = MPI_Wtime(); totalTime = finishTime - startTime; if (myMPIRank==0){ benchComplete = repTimeCheck(totalTime, repsToDo); } MPI_Bcast(&benchComplete, 1, MPI_INT, 0, comm); MPI_Bcast(&repsToDo, 1, MPI_INT, 0, comm); } /* End of loop to check if benchComplete is true */ if (myMPIRank == 0){ setReportParams(dataSizeIter, repsToDo, totalTime); printReport(); } freeMultiPingpongData(); dataSizeIter = dataSizeIter * 2; } /* end loop over data sizes */ return 0; }
int masteronlyMultiPingpong(int totalReps, int dataSize){ int repIter, i; for (repIter = 1; repIter <= totalReps; repIter++){ if (crossCommRank == pingNode){ #pragma omp parallel for default(none) \ private(i) \ shared(pingSendBuf,dataSize,sizeofBuffer,globalIDarray) \ schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ pingSendBuf[i] = globalIDarray[myThreadID]; } MPI_Send(pingSendBuf, sizeofBuffer, MPI_INT, pongNode, TAG, crossComm); MPI_Recv(pongRecvBuf, sizeofBuffer, MPI_INT, pongNode, \ TAG, crossComm, &status); #pragma omp parallel for default(none) \ private(i) \ shared(pongRecvBuf,finalRecvBuf,dataSize,sizeofBuffer) \ schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ finalRecvBuf[i] = pongRecvBuf[i]; } } else if (crossCommRank == pongNode){ MPI_Recv(pingRecvBuf, sizeofBuffer, MPI_INT, pingNode,\ TAG, crossComm, &status); #pragma omp parallel for default(none) \ private(i) \ shared(pongSendBuf,pingRecvBuf,dataSize,sizeofBuffer) \ schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ pongSendBuf[i] = pingRecvBuf[i]; } MPI_Send(pongSendBuf, sizeofBuffer, MPI_INT, pingNode, \ TAG, crossComm); } } return 0; }
int funnelledMultiPingpong(int totalReps, int dataSize){ int repIter, i; #pragma omp parallel default(none) \ private(i,repIter) \ shared(pingNode,pongNode,pingSendBuf,pingRecvBuf) \ shared(pongSendBuf,pongRecvBuf,finalRecvBuf,sizeofBuffer) \ shared(dataSize,globalIDarray,crossComm,status) \ shared(totalReps,myMPIRank,crossCommRank) { for (repIter = 1; repIter <= totalReps; repIter++){ if (crossCommRank == pingNode){ #pragma omp for schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ pingSendBuf[i] = globalIDarray[myThreadID]; } #pragma omp master { MPI_Send(pingSendBuf, sizeofBuffer, MPI_INT, pongNode, TAG, crossComm); MPI_Recv(pongRecvBuf, sizeofBuffer, MPI_INT, pongNode, TAG, \ crossComm, &status); } #pragma omp barrier #pragma omp for schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ finalRecvBuf[i] = pongRecvBuf[i]; } } else if (crossCommRank == pongNode){ #pragma omp master { MPI_Recv(pingRecvBuf, sizeofBuffer, MPI_INT, pingNode,\ TAG, crossComm, &status); } #pragma omp barrier #pragma omp for schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ pongSendBuf[i] = pingRecvBuf[i]; } #pragma omp master { MPI_Send(pongSendBuf, sizeofBuffer, MPI_INT, pingNode, TAG, crossComm); } } } } return 0; }
int multipleMultiPingpong(int totalReps, int dataSize){ int repIter, i; int lBound; #pragma omp parallel default(none) \ private(i,repIter,status,lBound) \ shared(pingNode,pongNode,pingSendBuf,pingRecvBuf) \ shared(pongSendBuf,pongRecvBuf,finalRecvBuf,sizeofBuffer) \ shared(dataSize,globalIDarray,crossComm) \ shared(totalReps,myMPIRank,crossCommRank) { for (repIter=1; repIter<=totalReps; repIter++){ /* loop totalRep times */ if (crossCommRank == pingNode){ lBound = (myThreadID * dataSize); #pragma omp for nowait schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ pingSendBuf[i] = globalIDarray[myThreadID]; } MPI_Send(&pingSendBuf[lBound], dataSize, MPI_INT, pongNode, \ myThreadID, crossComm); MPI_Recv(&pongRecvBuf[lBound], dataSize, MPI_INT, pongNode, \ myThreadID, crossComm, &status); #pragma omp for nowait schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ finalRecvBuf[i] = pongRecvBuf[i]; } } else if (crossCommRank == pongNode){ lBound = (myThreadID * dataSize); MPI_Recv(&pingRecvBuf[lBound], dataSize, MPI_INT, pingNode, \ myThreadID, crossComm, &status); #pragma omp for nowait schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ pongSendBuf[i] = pingRecvBuf[i]; } MPI_Send(&pongSendBuf[lBound], dataSize, MPI_INT, pingNode, \ myThreadID, crossComm); } } } return 0; }
int allocateMultiPingpongData(int sizeofBuffer){ if (crossCommRank == pingNode){ pingSendBuf = (int *)malloc(sizeof(int) * sizeofBuffer); pongRecvBuf = (int *)malloc(sizeof(int) * sizeofBuffer); finalRecvBuf = (int *)malloc(sizeof(int) * sizeofBuffer); } else if (crossCommRank == pongNode){ pingRecvBuf = (int *)malloc(sizeof(int) * sizeofBuffer); pongSendBuf = (int *)malloc(sizeof(int) * sizeofBuffer); } return 0; }
int freeMultiPingpongData(){ if (crossCommRank == pingNode){ free(pingSendBuf); free(pongRecvBuf); free(finalRecvBuf); } else if (crossCommRank == pongNode){ free(pingRecvBuf); free(pongSendBuf); } return 0; }
int testMultiPingpong(int sizeofBuffer, int dataSize){ int i; int testFlag, localTestFlag; localTestFlag = TRUE; if (crossCommRank == pingNode){ testBuf = (int *)malloc(sizeof(int) * sizeofBuffer); #pragma omp parallel for default(none) \ private(i) \ shared(testBuf,dataSize,sizeofBuffer,globalIDarray)\ schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ testBuf[i] = globalIDarray[myThreadID]; } for (i=0; i<sizeofBuffer; i++){ if (testBuf[i] != finalRecvBuf[i]){ localTestFlag = FALSE; } } free(testBuf); } MPI_Reduce(&localTestFlag, &testFlag, 1, MPI_INT,MPI_LAND, 0, comm); if (myMPIRank == 0){ setTestOutcome(testFlag); } return 0; }
int pingPing(int benchmarkType){ int dataSizeIter; int sameNode; pingRankA = PPRanks[0]; pingRankB = PPRanks[1]; sameNode = compareProcNames(pingRankA, pingRankB); if (myMPIRank == 0){ printNodeReport(sameNode,pingRankA,pingRankB); printBenchHeader(); } repsToDo = defaultReps; dataSizeIter = minDataSize; /* initialise dataSizeIter to minDataSize */ while (dataSizeIter <= maxDataSize){ sizeofBuffer = dataSizeIter * numThreads; allocatePingpingData(sizeofBuffer); if (benchmarkType == MASTERONLY){ masteronlyPingping(warmUpIters, dataSizeIter); } else if (benchmarkType == FUNNELLED){ funnelledPingping(warmUpIters, dataSizeIter); } else if (benchmarkType == MULTIPLE){ multiplePingping(warmUpIters, dataSizeIter); } testPingping(sizeofBuffer, dataSizeIter); benchComplete = FALSE; while (benchComplete != TRUE){ MPI_Barrier(comm); startTime = MPI_Wtime(); if (benchmarkType == MASTERONLY){ masteronlyPingping(repsToDo, dataSizeIter); } else if (benchmarkType == FUNNELLED){ funnelledPingping(repsToDo, dataSizeIter); } else if (benchmarkType == MULTIPLE){ multiplePingping(repsToDo, dataSizeIter); } MPI_Barrier(comm); finishTime = MPI_Wtime(); totalTime = finishTime - startTime; if (myMPIRank==0){ benchComplete = repTimeCheck(totalTime, repsToDo); } MPI_Bcast(&benchComplete, 1, MPI_INT, 0, comm); MPI_Bcast(&repsToDo, 1, MPI_INT, 0, comm); } if (myMPIRank == 0){ setReportParams(dataSizeIter, repsToDo, totalTime); printReport(); } freePingpingData(); dataSizeIter = dataSizeIter * 2; /* double data size */ } return 0; }
int masteronlyPingping(int totalReps, int dataSize){ int repIter, i; int destRank; if (myMPIRank == pingRankA){ destRank = pingRankB; } else if (myMPIRank == pingRankB){ destRank = pingRankA; } for (repIter = 0; repIter < totalReps; repIter++){ if (myMPIRank == pingRankA || myMPIRank == pingRankB){ #pragma omp parallel for default(none) \ private(i) \ shared(pingSendBuf,dataSize,sizeofBuffer,globalIDarray) \ schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ pingSendBuf[i] = globalIDarray[myThreadID]; } MPI_Isend(pingSendBuf, sizeofBuffer, MPI_INT, destRank, \ TAG, comm, &requestID); MPI_Recv(pingRecvBuf, sizeofBuffer, MPI_INT, destRank, \ TAG, comm, &status); MPI_Wait(&requestID, &status); #pragma omp parallel for default(none) \ private(i) \ shared(finalRecvBuf,dataSize,sizeofBuffer,pingRecvBuf) \ schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ finalRecvBuf[i] = pingRecvBuf[i]; } } } return 0; }






