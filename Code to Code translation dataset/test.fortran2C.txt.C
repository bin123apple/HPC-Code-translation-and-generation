int funnelledPingping(int totalReps, int dataSize){ int repIter, i; int destRank; if (myMPIRank == pingRankA){ destRank = pingRankB; } else if (myMPIRank == pingRankB){ destRank = pingRankA; } #pragma omp parallel default(none) \ private(i, repIter) \ shared(dataSize,sizeofBuffer,pingSendBuf,globalIDarray) \ shared(pingRecvBuf,finalRecvBuf,status,requestID) \ shared(destRank,comm,myMPIRank,pingRankA,pingRankB,totalReps) for (repIter = 0; repIter < totalReps; repIter++){ if (myMPIRank == pingRankA || myMPIRank == pingRankB){ #pragma omp for schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ pingSendBuf[i] = globalIDarray[myThreadID]; } #pragma omp master { MPI_Isend(pingSendBuf, sizeofBuffer, MPI_INT, destRank, \ TAG, comm, &requestID); MPI_Recv(pingRecvBuf, sizeofBuffer, MPI_INT, destRank, \ TAG, comm, &status); MPI_Wait(&requestID, &status); } #pragma omp barrier #pragma omp for schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ finalRecvBuf[i] = pingRecvBuf[i]; } } } return 0; }
int multiplePingping(int totalReps, int dataSize){ int repIter, i; int destRank; int lBound; if (myMPIRank == pingRankA){ destRank = pingRankB; } else if (myMPIRank == pingRankB){ destRank = pingRankA; } #pragma omp parallel default(none) \ private(i,lBound,requestID,status,repIter) \ shared(pingSendBuf,pingRecvBuf,finalRecvBuf,sizeofBuffer) \ shared(destRank,myMPIRank,pingRankA,pingRankB,totalReps) \ shared(dataSize,globalIDarray,comm) { for (repIter = 0; repIter < totalReps; repIter++){ if (myMPIRank == pingRankA || myMPIRank == pingRankB){ lBound = (myThreadID * dataSize); #pragma omp for nowait schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ pingSendBuf[i] = globalIDarray[myThreadID]; } MPI_Isend(&pingSendBuf[lBound], dataSize, MPI_INT, destRank, \ myThreadID, comm, &requestID); MPI_Recv(&pingRecvBuf[lBound], dataSize, MPI_INT, destRank, \ myThreadID, comm, &status); MPI_Wait(&requestID, &status); #pragma omp for nowait schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ finalRecvBuf[i] = pingRecvBuf[i]; } } } } return 0; }
int allocatePingpingData(int sizeofBuffer){ pingSendBuf = (int *)malloc(sizeofBuffer * sizeof(int)); pingRecvBuf = (int *)malloc(sizeofBuffer * sizeof(int)); finalRecvBuf = (int *)malloc(sizeofBuffer * sizeof(int)); return 0; }
int freePingpingData(){ free(pingSendBuf); free(pingRecvBuf); free(finalRecvBuf); return 0; }
int testPingping(int sizeofBuffer,int dataSize){ int otherPingRank, i, testFlag, reduceFlag; int *testBuf; testFlag = TRUE; if (myMPIRank == pingRankA || myMPIRank == pingRankB){ testBuf = (int *)malloc(sizeofBuffer * sizeof(int)); if (myMPIRank == pingRankA){ otherPingRank = pingRankB; } else if (myMPIRank == pingRankB){ otherPingRank = pingRankA; } #pragma omp parallel for default(none) \ private(i) \ shared(otherPingRank,numThreads,testBuf,dataSize,sizeofBuffer) \ schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ testBuf[i] = (otherPingRank * numThreads) + myThreadID; } for (i=0; i<sizeofBuffer; i++){ if (testBuf[i] != finalRecvBuf[i]){ testFlag = FALSE; } } free(testBuf); } MPI_Reduce(&testFlag, &reduceFlag, 1, MPI_INT, MPI_LAND, 0, comm); if (myMPIRank == 0){ setTestOutcome(reduceFlag); } return 0; }
int pingPong(int benchmarkType){ int dataSizeIter; int sameNode; pingRank = PPRanks[0]; pongRank = PPRanks[1]; sameNode = compareProcNames(pingRank,pongRank); if (myMPIRank == 0){ printNodeReport(sameNode,pingRank,pongRank); printBenchHeader(); } repsToDo = defaultReps; dataSizeIter = minDataSize; /* initialise dataSizeIter to minDataSize */ while (dataSizeIter <= maxDataSize){ sizeofBuffer = dataSizeIter * numThreads; allocatePingpongData(sizeofBuffer); if (benchmarkType == MASTERONLY){ masteronlyPingpong(warmUpIters, dataSizeIter); } else if (benchmarkType == FUNNELLED){ funnelledPingpong(warmUpIters, dataSizeIter); } else if (benchmarkType == MULTIPLE){ multiplePingpong(warmUpIters, dataSizeIter); } testPingpong(sizeofBuffer, dataSizeIter); benchComplete = FALSE; while (benchComplete != TRUE){ MPI_Barrier(comm); startTime = MPI_Wtime(); if (benchmarkType == MASTERONLY){ masteronlyPingpong(repsToDo, dataSizeIter); } else if (benchmarkType == FUNNELLED){ funnelledPingpong(repsToDo, dataSizeIter); } else if (benchmarkType == MULTIPLE){ multiplePingpong(repsToDo, dataSizeIter); } MPI_Barrier(comm); finishTime = MPI_Wtime(); totalTime = finishTime - startTime; if (myMPIRank==0){ benchComplete = repTimeCheck(totalTime, repsToDo); } MPI_Bcast(&benchComplete, 1, MPI_INT, 0, comm); MPI_Bcast(&repsToDo, 1, MPI_INT, 0, comm); } if (myMPIRank == 0){ setReportParams(dataSizeIter, repsToDo, totalTime); printReport(); } freePingpongData(); dataSizeIter = dataSizeIter * 2; } return 0; }
int masteronlyPingpong(int totalReps, int dataSize){ int repIter, i; for (repIter = 0; repIter < totalReps; repIter++){ * write to their part of the pingBuf array using a * parallel for directive. */ if (myMPIRank == pingRank){ #pragma omp parallel for default(none) \ private(i) \ shared(pingSendBuf,dataSize,sizeofBuffer,globalIDarray) \ schedule(static,dataSize) for(i=0; i<sizeofBuffer; i++){ pingSendBuf[i] = globalIDarray[myThreadID]; } MPI_Send(pingSendBuf, sizeofBuffer, MPI_INT, pongRank, TAG, comm); MPI_Recv(pongRecvBuf, sizeofBuffer, MPI_INT, pongRank, \ TAG, comm, &status); #pragma omp parallel for default(none) \ private(i) \ shared(pongRecvBuf,finalRecvBuf,dataSize,sizeofBuffer) \ schedule(static,dataSize) for(i=0; i<sizeofBuffer; i++){ finalRecvBuf[i] = pongRecvBuf[i]; } } else if (myMPIRank == pongRank){ MPI_Recv(pingRecvBuf, sizeofBuffer, MPI_INT, pingRank, \ TAG, comm, &status); #pragma omp parallel for default(none) \ private(i) \ shared(pongSendBuf,pingRecvBuf,dataSize,sizeofBuffer) \ schedule(static,dataSize) for(i=0; i< sizeofBuffer; i++){ pongSendBuf[i] = pingRecvBuf[i]; } MPI_Send(pongSendBuf, sizeofBuffer, MPI_INTEGER, pingRank, \ TAG, comm); } } return 0; }
int funnelledPingpong(int totalReps, int dataSize){ int repIter, i; #pragma omp parallel default(none) \ private(i,repIter) \ shared(pingRank,pongRank,pingSendBuf,pingRecvBuf) \ shared(pongSendBuf,pongRecvBuf,finalRecvBuf,sizeofBuffer) \ shared(dataSize,globalIDarray,comm,status,totalReps,myMPIRank) { for (repIter=0; repIter< totalReps; repIter++){ if (myMPIRank == pingRank){ #pragma omp for schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ pingSendBuf[i] = globalIDarray[myThreadID]; } #pragma omp master { MPI_Send(pingSendBuf, sizeofBuffer, MPI_INT, pongRank, TAG, comm); MPI_Recv(pongRecvBuf, sizeofBuffer, MPI_INT, pongRank, TAG, \ comm, &status); } #pragma omp barrier #pragma omp for schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ finalRecvBuf[i] = pongRecvBuf[i]; } } else if (myMPIRank == pongRank){ #pragma omp master { MPI_Recv(pingRecvBuf, sizeofBuffer, MPI_INT, pingRank, TAG, comm, &status); } #pragma omp barrier #pragma omp for schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ pongSendBuf[i] = pingRecvBuf[i]; } #pragma omp master { MPI_Send(pongSendBuf, sizeofBuffer, MPI_INT, pingRank, TAG, comm); } } } /* end of repetitions */ } /* end of parallel region */ return 0; }
int multiplePingpong(int totalReps, int dataSize){ int repIter, i, lBound, uBound; #pragma omp parallel default(none) \ private(i,repIter,lBound) \ shared(pingRank,pongRank,pingSendBuf,pingRecvBuf) \ shared(pongSendBuf,pongRecvBuf,finalRecvBuf,sizeofBuffer) \ shared(dataSize,globalIDarray,comm,status,totalReps,myMPIRank) { for (repIter=0; repIter < totalReps; repIter++){ if (myMPIRank == pingRank){ lBound = (myThreadID * dataSize); #pragma omp for nowait schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ pingSendBuf[i] = globalIDarray[myThreadID]; } MPI_Send(&pingSendBuf[lBound], dataSize, MPI_INT, pongRank, \ myThreadID, comm); MPI_Recv(&pongRecvBuf[lBound], dataSize, MPI_INT, pongRank, \ myThreadID, comm, &status); #pragma omp for nowait schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ finalRecvBuf[i] = pongRecvBuf[i]; } } else if (myMPIRank == pongRank){ lBound = (myThreadID * dataSize); MPI_Recv(&pingRecvBuf[lBound], dataSize, MPI_INT, pingRank, \ myThreadID, comm, &status); #pragma omp for nowait schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++) { pongSendBuf[i] = pingRecvBuf[i]; } MPI_Send(&pongSendBuf[lBound], dataSize, MPI_INT, pingRank, \ myThreadID, comm); } } } return 0; }
int allocatePingpongData(int sizeofBuffer){ pingSendBuf = (int *)malloc(sizeofBuffer * sizeof(int)); pingRecvBuf = (int *)malloc(sizeofBuffer * sizeof(int)); pongSendBuf = (int *)malloc(sizeofBuffer * sizeof(int)); pongRecvBuf = (int *)malloc(sizeofBuffer * sizeof(int)); finalRecvBuf = (int *)malloc(sizeofBuffer * sizeof(int)); return 0; }
int freePingpongData(){ free(pingSendBuf); free(pingRecvBuf); free(pongSendBuf); free(pongRecvBuf); free(finalRecvBuf); return 0; }
int testPingpong(int sizeofBuffer,int dataSize){ int i, testFlag; int *testBuf; if (myMPIRank == pingRank){ testFlag = TRUE; testBuf = (int *)malloc(sizeofBuffer * sizeof(int)); #pragma omp parallel for default(none) \ private(i) \ shared(testBuf,dataSize,sizeofBuffer,globalIDarray) \ schedule(static,dataSize) for (i=0; i<sizeofBuffer; i++){ testBuf[i] = globalIDarray[myThreadID]; } for (i=0; i<sizeofBuffer; i++){ if (testBuf[i] != finalRecvBuf[i]){ testFlag = FALSE; } } free(testBuf); } MPI_Bcast(&testFlag, 1, MPI_INT, pingRank, comm); if (myMPIRank == 0){ setTestOutcome(testFlag); } return 0; }





























